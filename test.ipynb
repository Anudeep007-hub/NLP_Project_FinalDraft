{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CleanText import SentTokenize\n",
    "from  RemoveStopWords import stopWords_removal\n",
    "from PorterStemmer import porter_stemmer\n",
    "import csv, json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 02:25:48.249295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-16 02:25:48.266324: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731704148.286555  375563 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731704148.292667  375563 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 02:25:48.313813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(input_phrase):\n",
    "    obj = SentTokenize(phrase=input_phrase) \n",
    "    obj = obj.get_tokenized_sentence() \n",
    "    print(f\"Tokenized sentence: {obj}\")\n",
    "\n",
    "    obj = stopWords_removal(tokenized_sentence=obj) \n",
    "    obj = obj.get_refined_tokeinzed_sentence()\n",
    "    print(f\"After stop words removal: {obj}\")\n",
    "\n",
    "    obj = porter_stemmer(tokenised_phrase=obj)\n",
    "    obj = obj.get_stemmed_tokens() \n",
    "    print(f\"After stemming with porters algorithm: {obj}\") \n",
    "\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['why', 'can', 't', 'i', 'connect', 'to', 'wifi']\n",
      "After stop words removal: ['connect', 'wifi']\n",
      "After stemming with porters algorithm: ['connect', 'wifi']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'wifi', 'connection', 'issues']\n",
      "After stop words removal: ['fix', 'wifi', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'wifi', 'connect', 'issu']\n",
      "Tokenized sentence: ['my', 'wifi', 'is', 'not', 'working']\n",
      "After stop words removal: ['wifi', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['wifi', 'wor']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'wifi', 'connection', 'unstable']\n",
      "After stop words removal: ['wifi', 'connection', 'unstable']\n",
      "After stemming with porters algorithm: ['wifi', 'connect', 'unstab']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reconnect', 'to', 'wifi', 'after', 'losing', 'connection']\n",
      "After stop words removal: ['reconnect', 'wifi', 'losing', 'connection']\n",
      "los\n",
      "After stemming with porters algorithm: ['reconnect', 'wifi', 'lose', 'connect']\n",
      "Tokenized sentence: ['what', 'should', 'i', 'do', 'when', 'wifi', 'won', 't', 'connect']\n",
      "After stop words removal: ['wifi', 'connect']\n",
      "After stemming with porters algorithm: ['wifi', 'connect']\n",
      "Tokenized sentence: ['my', 'device', 'won', 't', 'connect', 'to', 'wifi', 'network']\n",
      "After stop words removal: ['device', 'connect', 'wifi', 'network']\n",
      "After stemming with porters algorithm: ['devic', 'connect', 'wifi', 'network']\n",
      "Tokenized sentence: ['wifi', 'connection', 'keeps', 'dropping']\n",
      "After stop words removal: ['wifi', 'connection', 'keeps', 'dropping']\n",
      "dropp\n",
      "After stemming with porters algorithm: ['wifi', 'connect', 'keep', 'drop']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'wifi', 'signal', 'weak']\n",
      "After stop words removal: ['wifi', 'signal', 'weak']\n",
      "After stemming with porters algorithm: ['wifi', 'signal', 'weak']\n",
      "Tokenized sentence: ['help', 'with', 'wifi', 'connectivity', 'problems']\n",
      "After stop words removal: ['help', 'wifi', 'connectivity', 'problems']\n",
      "After stemming with porters algorithm: ['help', 'wifi', 'connect', 'problem']\n",
      "Tokenized sentence: ['ram', 'frequency', 'not', 'being', 'detected', 'correctly']\n",
      "After stop words removal: ['ram', 'frequency', 'detected', 'correctly']\n",
      "After stemming with porters algorithm: ['ram', 'frequenc', 'detec', 'correctli']\n",
      "Tokenized sentence: ['ram', 'speed', 'issue']\n",
      "After stop words removal: ['ram', 'speed', 'issue']\n",
      "After stemming with porters algorithm: ['ram', 'speed', 'issu']\n",
      "Tokenized sentence: ['why', 'isn', 't', 'my', 'ram', 'running', 'at', 'rated', 'speed']\n",
      "After stop words removal: ['ram', 'running', 'rated', 'speed']\n",
      "runn\n",
      "rate\n",
      "After stemming with porters algorithm: ['ram', 'run', 'rate', 'speed']\n",
      "Tokenized sentence: ['ram', 'frequency', 'showing', 'wrong', 'in', 'bios']\n",
      "After stop words removal: ['ram', 'frequency', 'showing', 'wrong', 'bios']\n",
      "show\n",
      "After stemming with porters algorithm: ['ram', 'frequenc', 'showe', 'wrong', 'bio']\n",
      "Tokenized sentence: ['memory', 'speed', 'detection', 'problems']\n",
      "After stop words removal: ['memory', 'speed', 'detection', 'problems']\n",
      "After stemming with porters algorithm: ['memori', 'speed', 'detect', 'problem']\n",
      "Tokenized sentence: ['ram', 'running', 'slower', 'than', 'specified']\n",
      "After stop words removal: ['ram', 'running', 'slower', 'specified']\n",
      "runn\n",
      "After stemming with porters algorithm: ['ram', 'run', 'slower', 'specifi']\n",
      "Tokenized sentence: ['bios', 'not', 'detecting', 'correct', 'ram', 'frequency']\n",
      "After stop words removal: ['bios', 'detecting', 'correct', 'ram', 'frequency']\n",
      "detect\n",
      "After stemming with porters algorithm: ['bio', 'detec', 'correct', 'ram', 'frequenc']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'ram', 'frequency', 'detection']\n",
      "After stop words removal: ['fix', 'ram', 'frequency', 'detection']\n",
      "After stemming with porters algorithm: ['fix', 'ram', 'frequenc', 'detect']\n",
      "Tokenized sentence: ['ram', 'speed', 'lower', 'than', 'advertised']\n",
      "After stop words removal: ['ram', 'speed', 'lower', 'advertised']\n",
      "After stemming with porters algorithm: ['ram', 'speed', 'lower', 'advertis']\n",
      "Tokenized sentence: ['memory', 'clock', 'speed', 'detection', 'issue']\n",
      "After stop words removal: ['memory', 'clock', 'speed', 'detection', 'issue']\n",
      "After stemming with porters algorithm: ['memori', 'clock', 'speed', 'detect', 'issu']\n",
      "Tokenized sentence: ['power', 'supply', 'failing']\n",
      "After stop words removal: ['power', 'supply', 'failing']\n",
      "fail\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'fail']\n",
      "Tokenized sentence: ['psu', 'problems']\n",
      "After stop words removal: ['psu', 'problems']\n",
      "After stemming with porters algorithm: ['psu', 'problem']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'power', 'supply', 'not', 'working']\n",
      "After stop words removal: ['power', 'supply', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'wor']\n",
      "Tokenized sentence: ['psu', 'making', 'strange', 'noises']\n",
      "After stop words removal: ['psu', 'making', 'strange', 'noises']\n",
      "mak\n",
      "After stemming with porters algorithm: ['psu', 'make', 'strang', 'nois']\n",
      "Tokenized sentence: ['computer', 'randomly', 'shutting', 'down', 'psu']\n",
      "After stop words removal: ['computer', 'randomly', 'shutting', 'psu']\n",
      "shutt\n",
      "After stemming with porters algorithm: ['comput', 'randomli', 'shut', 'psu']\n",
      "Tokenized sentence: ['power', 'supply', 'unit', 'failure', 'symptoms']\n",
      "After stop words removal: ['power', 'supply', 'unit', 'failure', 'symptoms']\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'unit', 'failur', 'symptom']\n",
      "Tokenized sentence: ['how', 'to', 'diagnose', 'psu', 'issues']\n",
      "After stop words removal: ['diagnose', 'psu', 'issues']\n",
      "After stemming with porters algorithm: ['diagnos', 'psu', 'issu']\n",
      "Tokenized sentence: ['psu', 'showing', 'signs', 'of', 'failure']\n",
      "After stop words removal: ['psu', 'showing', 'signs', 'failure']\n",
      "show\n",
      "After stemming with porters algorithm: ['psu', 'showe', 'sign', 'failur']\n",
      "Tokenized sentence: ['power', 'supply', 'unstable', 'behavior']\n",
      "After stop words removal: ['power', 'supply', 'unstable', 'behavior']\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'unstab', 'behavior']\n",
      "Tokenized sentence: ['psu', 'reliability', 'problems']\n",
      "After stop words removal: ['psu', 'reliability', 'problems']\n",
      "After stemming with porters algorithm: ['psu', 'reliab', 'problem']\n",
      "Tokenized sentence: ['motherboard', 'bios', 'not', 'detecting', 'new', 'cpu']\n",
      "After stop words removal: ['motherboard', 'bios', 'detecting', 'new', 'cpu']\n",
      "detect\n",
      "After stemming with porters algorithm: ['motherboard', 'bio', 'detec', 'new', 'cpu']\n",
      "Tokenized sentence: ['cpu', 'not', 'recognized']\n",
      "After stop words removal: ['cpu', 'recognized']\n",
      "recognize\n",
      "After stemming with porters algorithm: ['cpu', 'recogn']\n",
      "Tokenized sentence: ['bios', 'fails', 'to', 'detect', 'processor']\n",
      "After stop words removal: ['bios', 'fails', 'detect', 'processor']\n",
      "After stemming with porters algorithm: ['bio', 'fail', 'detect', 'processor']\n",
      "Tokenized sentence: ['new', 'cpu', 'installation', 'detection', 'problem']\n",
      "After stop words removal: ['new', 'cpu', 'installation', 'detection', 'problem']\n",
      "After stemming with porters algorithm: ['new', 'cpu', 'instal', 'detect', 'problem']\n",
      "Tokenized sentence: ['why', 'isn', 't', 'my', 'cpu', 'being', 'detected']\n",
      "After stop words removal: ['cpu', 'detected']\n",
      "After stemming with porters algorithm: ['cpu', 'detec']\n",
      "Tokenized sentence: ['motherboard', 'can', 't', 'see', 'new', 'processor']\n",
      "After stop words removal: ['motherboard', 'see', 'new', 'processor']\n",
      "After stemming with porters algorithm: ['motherboard', 'see', 'new', 'processor']\n",
      "Tokenized sentence: ['cpu', 'not', 'showing', 'up', 'in', 'bios']\n",
      "After stop words removal: ['cpu', 'showing', 'bios']\n",
      "show\n",
      "After stemming with porters algorithm: ['cpu', 'showe', 'bio']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'cpu', 'detection', 'issues']\n",
      "After stop words removal: ['fix', 'cpu', 'detection', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'cpu', 'detect', 'issu']\n",
      "Tokenized sentence: ['bios', 'cpu', 'recognition', 'problem']\n",
      "After stop words removal: ['bios', 'cpu', 'recognition', 'problem']\n",
      "After stemming with porters algorithm: ['bio', 'cpu', 'recognit', 'problem']\n",
      "Tokenized sentence: ['troubleshoot', 'cpu', 'detection']\n",
      "After stop words removal: ['troubleshoot', 'cpu', 'detection']\n",
      "After stemming with porters algorithm: ['troubleshoot', 'cpu', 'detect']\n",
      "Tokenized sentence: ['how', 'to', 'check', 'wifi', 'security', 'settings']\n",
      "After stop words removal: ['check', 'wifi', 'security', 'settings']\n",
      "sett\n",
      "After stemming with porters algorithm: ['check', 'wifi', 'secur', 'set']\n",
      "Tokenized sentence: ['wifi', 'security', 'type']\n",
      "After stop words removal: ['wifi', 'security', 'type']\n",
      "After stemming with porters algorithm: ['wifi', 'secur', 'type']\n",
      "Tokenized sentence: ['is', 'my', 'wifi', 'secure']\n",
      "After stop words removal: ['wifi', 'secure']\n",
      "After stemming with porters algorithm: ['wifi', 'secur']\n",
      "Tokenized sentence: ['what', 'type', 'of', 'security', 'is', 'my', 'wifi', 'using']\n",
      "After stop words removal: ['type', 'security', 'wifi', 'using']\n",
      "us\n",
      "After stemming with porters algorithm: ['type', 'secur', 'wifi', 'us']\n",
      "Tokenized sentence: ['check', 'wpa', 'or', 'wpa', 'on', 'wifi']\n",
      "After stop words removal: ['check', 'wpa', 'wpa', 'wifi']\n",
      "After stemming with porters algorithm: ['check', 'wpa', 'wpa', 'wifi']\n",
      "Tokenized sentence: ['how', 'to', 'verify', 'wifi', 'security', 'protocol']\n",
      "After stop words removal: ['verify', 'wifi', 'security', 'protocol']\n",
      "After stemming with porters algorithm: ['verifi', 'wifi', 'secur', 'protocol']\n",
      "Tokenized sentence: ['ways', 'to', 'check', 'wireless', 'security', 'settings']\n",
      "After stop words removal: ['ways', 'check', 'wireless', 'security', 'settings']\n",
      "sett\n",
      "After stemming with porters algorithm: ['wai', 'check', 'wireless', 'secur', 'set']\n",
      "Tokenized sentence: ['finding', 'wifi', 'encryption', 'type']\n",
      "After stop words removal: ['finding', 'wifi', 'encryption', 'type']\n",
      "find\n",
      "After stemming with porters algorithm: ['fin', 'wifi', 'encrypt', 'type']\n",
      "Tokenized sentence: ['determine', 'wifi', 'security', 'standard']\n",
      "After stop words removal: ['determine', 'wifi', 'security', 'standard']\n",
      "After stemming with porters algorithm: ['determin', 'wifi', 'secur', 'standard']\n",
      "Tokenized sentence: ['view', 'network', 'security', 'settings']\n",
      "After stop words removal: ['view', 'network', 'security', 'settings']\n",
      "sett\n",
      "After stemming with porters algorithm: ['view', 'network', 'secur', 'set']\n",
      "Tokenized sentence: ['extremely', 'quiet', 'but', 'constant', 'buzzing', 'noises', 'when', 'the', 'power', 'cable', 'is', 'plugged', 'in']\n",
      "After stop words removal: ['extremely', 'quiet', 'constant', 'buzzing', 'noises', 'power', 'cable', 'plugged']\n",
      "buzz\n",
      "After stemming with porters algorithm: ['extrem', 'quiet', 'constant', 'buzz', 'nois', 'power', 'cabl', 'plug']\n",
      "Tokenized sentence: ['buzzing', 'sound', 'when', 'power', 'cable', 'is', 'connected']\n",
      "After stop words removal: ['buzzing', 'sound', 'power', 'cable', 'connected']\n",
      "buzz\n",
      "After stemming with porters algorithm: ['buzz', 'sound', 'power', 'cabl', 'connec']\n",
      "Tokenized sentence: ['power', 'supply', 'making', 'quiet', 'buzzing', 'noise']\n",
      "After stop words removal: ['power', 'supply', 'making', 'quiet', 'buzzing', 'noise']\n",
      "mak\n",
      "buzz\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'make', 'quiet', 'buzz', 'nois']\n",
      "Tokenized sentence: ['faint', 'buzzing', 'from', 'pc', 'power', 'cable']\n",
      "After stop words removal: ['faint', 'buzzing', 'pc', 'power', 'cable']\n",
      "buzz\n",
      "After stemming with porters algorithm: ['faint', 'buzz', 'power', 'cabl']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'power', 'cable', 'buzzing']\n",
      "After stop words removal: ['power', 'cable', 'buzzing']\n",
      "buzz\n",
      "After stemming with porters algorithm: ['power', 'cabl', 'buzz']\n",
      "Tokenized sentence: ['computer', 'making', 'quiet', 'electrical', 'noise']\n",
      "After stop words removal: ['computer', 'making', 'quiet', 'electrical', 'noise']\n",
      "mak\n",
      "After stemming with porters algorithm: ['comput', 'make', 'quiet', 'electr', 'nois']\n",
      "Tokenized sentence: ['psu', 'cable', 'creating', 'buzzing', 'sound']\n",
      "After stop words removal: ['psu', 'cable', 'creating', 'buzzing', 'sound']\n",
      "creat\n",
      "create\n",
      "buzz\n",
      "After stemming with porters algorithm: ['psu', 'cabl', 'creat', 'buzz', 'sound']\n",
      "Tokenized sentence: ['constant', 'electrical', 'humming', 'from', 'power', 'cable']\n",
      "After stop words removal: ['constant', 'electrical', 'humming', 'power', 'cable']\n",
      "humm\n",
      "After stemming with porters algorithm: ['constant', 'electr', 'hum', 'power', 'cabl']\n",
      "Tokenized sentence: ['low', 'buzzing', 'noise', 'from', 'pc', 'power', 'connection']\n",
      "After stop words removal: ['low', 'buzzing', 'noise', 'pc', 'power', 'connection']\n",
      "buzz\n",
      "After stemming with porters algorithm: ['low', 'buzz', 'nois', 'power', 'connect']\n",
      "Tokenized sentence: ['power', 'connector', 'making', 'buzzing', 'sounds']\n",
      "After stop words removal: ['power', 'connector', 'making', 'buzzing', 'sounds']\n",
      "mak\n",
      "buzz\n",
      "After stemming with porters algorithm: ['power', 'connector', 'make', 'buzz', 'sound']\n",
      "Tokenized sentence: ['mobo', 'keeps', 'flashing', 'gpu', 'troubleshoot', 'light']\n",
      "After stop words removal: ['mobo', 'keeps', 'flashing', 'gpu', 'troubleshoot', 'light']\n",
      "flash\n",
      "After stemming with porters algorithm: ['mobo', 'keep', 'flas', 'gpu', 'troubleshoot', 'light']\n",
      "Tokenized sentence: ['gpu', 'light', 'on', 'mobo', 'flashing']\n",
      "After stop words removal: ['gpu', 'light', 'mobo', 'flashing']\n",
      "flash\n",
      "After stemming with porters algorithm: ['gpu', 'light', 'mobo', 'flas']\n",
      "Tokenized sentence: ['motherboard', 'gpu', 'indicator', 'light', 'blinking']\n",
      "After stop words removal: ['motherboard', 'gpu', 'indicator', 'light', 'blinking']\n",
      "blink\n",
      "After stemming with porters algorithm: ['motherboard', 'gpu', 'indic', 'light', 'blin']\n",
      "Tokenized sentence: ['gpu', 'debug', 'led', 'flashing', 'on', 'motherboard']\n",
      "After stop words removal: ['gpu', 'debug', 'led', 'flashing', 'motherboard']\n",
      "flash\n",
      "After stemming with porters algorithm: ['gpu', 'debug', 'led', 'flas', 'motherboard']\n",
      "Tokenized sentence: ['graphics', 'card', 'warning', 'light', 'on', 'motherboard']\n",
      "After stop words removal: ['graphics', 'card', 'warning', 'light', 'motherboard']\n",
      "warn\n",
      "After stemming with porters algorithm: ['graphic', 'card', 'war', 'light', 'motherboard']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'gpu', 'light', 'flashing']\n",
      "After stop words removal: ['gpu', 'light', 'flashing']\n",
      "flash\n",
      "After stemming with porters algorithm: ['gpu', 'light', 'flas']\n",
      "Tokenized sentence: ['motherboard', 'showing', 'gpu', 'problem', 'light']\n",
      "After stop words removal: ['motherboard', 'showing', 'gpu', 'problem', 'light']\n",
      "show\n",
      "After stemming with porters algorithm: ['motherboard', 'showe', 'gpu', 'problem', 'light']\n",
      "Tokenized sentence: ['gpu', 'error', 'light', 'keeps', 'blinking']\n",
      "After stop words removal: ['gpu', 'error', 'light', 'keeps', 'blinking']\n",
      "blink\n",
      "After stemming with porters algorithm: ['gpu', 'error', 'light', 'keep', 'blin']\n",
      "Tokenized sentence: ['graphics', 'card', 'troubleshooting', 'led', 'active']\n",
      "After stop words removal: ['graphics', 'card', 'troubleshooting', 'led', 'active']\n",
      "troubleshoot\n",
      "After stemming with porters algorithm: ['graphic', 'card', 'troubleshoot', 'led', 'activ']\n",
      "Tokenized sentence: ['gpu', 'diagnostic', 'light', 'won', 't', 'stop', 'flashing']\n",
      "After stop words removal: ['gpu', 'diagnostic', 'light', 'stop', 'flashing']\n",
      "flash\n",
      "After stemming with porters algorithm: ['gpu', 'diagnost', 'light', 'stop', 'flas']\n",
      "Tokenized sentence: ['monitor', 'loses', 'signal']\n",
      "After stop words removal: ['monitor', 'loses', 'signal']\n",
      "After stemming with porters algorithm: ['monitor', 'lose', 'signal']\n",
      "Tokenized sentence: ['signal', 'lost', 'on', 'monitor']\n",
      "After stop words removal: ['signal', 'lost', 'monitor']\n",
      "After stemming with porters algorithm: ['signal', 'lost', 'monitor']\n",
      "Tokenized sentence: ['display', 'suddenly', 'loses', 'connection']\n",
      "After stop words removal: ['display', 'suddenly', 'loses', 'connection']\n",
      "After stemming with porters algorithm: ['displai', 'suddenli', 'lose', 'connect']\n",
      "Tokenized sentence: ['monitor', 'going', 'black', 'randomly']\n",
      "After stop words removal: ['monitor', 'going', 'black', 'randomly']\n",
      "go\n",
      "After stemming with porters algorithm: ['monitor', 'go', 'black', 'randomli']\n",
      "Tokenized sentence: ['screen', 'losing', 'signal', 'intermittently']\n",
      "After stop words removal: ['screen', 'losing', 'signal', 'intermittently']\n",
      "los\n",
      "After stemming with porters algorithm: ['screen', 'lose', 'signal', 'intermitt']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'monitor', 'keep', 'losing', 'signal']\n",
      "After stop words removal: ['monitor', 'keep', 'losing', 'signal']\n",
      "los\n",
      "After stemming with porters algorithm: ['monitor', 'keep', 'lose', 'signal']\n",
      "Tokenized sentence: ['display', 'signal', 'dropping', 'frequently']\n",
      "After stop words removal: ['display', 'signal', 'dropping', 'frequently']\n",
      "dropp\n",
      "After stemming with porters algorithm: ['displai', 'signal', 'drop', 'frequent']\n",
      "Tokenized sentence: ['monitor', 'signal', 'connection', 'issues']\n",
      "After stop words removal: ['monitor', 'signal', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['monitor', 'signal', 'connect', 'issu']\n",
      "Tokenized sentence: ['screen', 'goes', 'black', 'no', 'signal']\n",
      "After stop words removal: ['screen', 'goes', 'black', 'signal']\n",
      "After stemming with porters algorithm: ['screen', 'goe', 'black', 'signal']\n",
      "Tokenized sentence: ['monitor', 'signal', 'loss', 'problem']\n",
      "After stop words removal: ['monitor', 'signal', 'loss', 'problem']\n",
      "After stemming with porters algorithm: ['monitor', 'signal', 'loss', 'problem']\n",
      "Tokenized sentence: ['memory', 'overclock', 'and', 'tuning', 'help']\n",
      "After stop words removal: ['memory', 'overclock', 'tuning', 'help']\n",
      "tun\n",
      "After stemming with porters algorithm: ['memori', 'overclock', 'tune', 'help']\n",
      "Tokenized sentence: ['help', 'with', 'memory', 'overclock', 'and', 'tuning']\n",
      "After stop words removal: ['help', 'memory', 'overclock', 'tuning']\n",
      "tun\n",
      "After stemming with porters algorithm: ['help', 'memori', 'overclock', 'tune']\n",
      "Tokenized sentence: ['how', 'to', 'tune', 'ram', 'overclock']\n",
      "After stop words removal: ['tune', 'ram', 'overclock']\n",
      "After stemming with porters algorithm: ['tune', 'ram', 'overclock']\n",
      "Tokenized sentence: ['memory', 'overclocking', 'assistance', 'needed']\n",
      "After stop words removal: ['memory', 'overclocking', 'assistance', 'needed']\n",
      "overclock\n",
      "After stemming with porters algorithm: ['memori', 'overcloc', 'assist', 'need']\n",
      "Tokenized sentence: ['ram', 'overclocking', 'guidelines']\n",
      "After stop words removal: ['ram', 'overclocking', 'guidelines']\n",
      "overclock\n",
      "After stemming with porters algorithm: ['ram', 'overcloc', 'guidelin']\n",
      "Tokenized sentence: ['best', 'settings', 'for', 'memory', 'overclocking']\n",
      "After stop words removal: ['best', 'settings', 'memory', 'overclocking']\n",
      "sett\n",
      "overclock\n",
      "After stemming with porters algorithm: ['best', 'set', 'memori', 'overcloc']\n",
      "Tokenized sentence: ['memory', 'timing', 'optimization', 'help']\n",
      "After stop words removal: ['memory', 'timing', 'optimization', 'help']\n",
      "tim\n",
      "After stemming with porters algorithm: ['memori', 'time', 'optimiz', 'help']\n",
      "Tokenized sentence: ['ram', 'overclock', 'stability', 'issues']\n",
      "After stop words removal: ['ram', 'overclock', 'stability', 'issues']\n",
      "After stemming with porters algorithm: ['ram', 'overclock', 'stabil', 'issu']\n",
      "Tokenized sentence: ['memory', 'overclocking', 'tips', 'needed']\n",
      "After stop words removal: ['memory', 'overclocking', 'tips', 'needed']\n",
      "overclock\n",
      "After stemming with porters algorithm: ['memori', 'overcloc', 'tip', 'need']\n",
      "Tokenized sentence: ['how', 'to', 'properly', 'tune', 'ram', 'timings']\n",
      "After stop words removal: ['properly', 'tune', 'ram', 'timings']\n",
      "tim\n",
      "After stemming with porters algorithm: ['properli', 'tune', 'ram', 'time']\n",
      "Tokenized sentence: ['ram', 'stick', 'issue']\n",
      "After stop words removal: ['ram', 'stick', 'issue']\n",
      "After stemming with porters algorithm: ['ram', 'stick', 'issu']\n",
      "Tokenized sentence: ['one', 'ram', 'stick', 'is', 'not', 'working']\n",
      "After stop words removal: ['one', 'ram', 'stick', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['on', 'ram', 'stick', 'wor']\n",
      "Tokenized sentence: ['memory', 'module', 'failure']\n",
      "After stop words removal: ['memory', 'module', 'failure']\n",
      "After stemming with porters algorithm: ['memori', 'modul', 'failur']\n",
      "Tokenized sentence: ['single', 'ram', 'stick', 'problem']\n",
      "After stop words removal: ['single', 'ram', 'stick', 'problem']\n",
      "After stemming with porters algorithm: ['singl', 'ram', 'stick', 'problem']\n",
      "Tokenized sentence: ['why', 'isn', 't', 'my', 'ram', 'working']\n",
      "After stop words removal: ['ram', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['ram', 'wor']\n",
      "Tokenized sentence: ['memory', 'stick', 'not', 'being', 'detected']\n",
      "After stop words removal: ['memory', 'stick', 'detected']\n",
      "After stemming with porters algorithm: ['memori', 'stick', 'detec']\n",
      "Tokenized sentence: ['ram', 'module', 'malfunction']\n",
      "After stop words removal: ['ram', 'module', 'malfunction']\n",
      "After stemming with porters algorithm: ['ram', 'modul', 'malfunct']\n",
      "Tokenized sentence: ['faulty', 'ram', 'stick', 'symptoms']\n",
      "After stop words removal: ['faulty', 'ram', 'stick', 'symptoms']\n",
      "After stemming with porters algorithm: ['faulti', 'ram', 'stick', 'symptom']\n",
      "Tokenized sentence: ['memory', 'stick', 'recognition', 'issue']\n",
      "After stop words removal: ['memory', 'stick', 'recognition', 'issue']\n",
      "After stemming with porters algorithm: ['memori', 'stick', 'recognit', 'issu']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'bad', 'ram', 'stick']\n",
      "After stop words removal: ['fix', 'bad', 'ram', 'stick']\n",
      "After stemming with porters algorithm: ['fix', 'bad', 'ram', 'stick']\n",
      "Tokenized sentence: ['i', 'have', 'laptop', 'hp', 's', 'eq', 'xxx', 'ryzen', 'u', 'currently', 'it', 'has', 'gb', 'ddr', 'mhz', 'i', 'want', 'to', 'grab', 'more', 'ram', 'stick', 'to', 'make', 'it', 'dual', 'channel', 'is', 'it', 'necessary', 'to', 'get', 'the', 'same', 'model']\n",
      "After stop words removal: ['laptop', 'hp', 'eq', 'xxx', 'ryzen', 'u', 'currently', 'gb', 'ddr', 'mhz', 'want', 'grab', 'ram', 'stick', 'make', 'dual', 'channel', 'necessary', 'get', 'model']\n",
      "After stemming with porters algorithm: ['laptop', 'xxx', 'ryzen', 'current', 'ddr', 'mhz', 'want', 'grab', 'ram', 'stick', 'make', 'dual', 'channel', 'necessari', 'get', 'model']\n",
      "Tokenized sentence: ['do', 'i', 'need', 'identical', 'ram', 'sticks', 'for', 'dual', 'channel']\n",
      "After stop words removal: ['need', 'identical', 'ram', 'sticks', 'dual', 'channel']\n",
      "After stemming with porters algorithm: ['need', 'ident', 'ram', 'stick', 'dual', 'channel']\n",
      "Tokenized sentence: ['can', 'i', 'mix', 'different', 'ram', 'models', 'in', 'dual', 'channel']\n",
      "After stop words removal: ['mix', 'different', 'ram', 'models', 'dual', 'channel']\n",
      "After stemming with porters algorithm: ['mix', 'differ', 'ram', 'model', 'dual', 'channel']\n",
      "Tokenized sentence: ['dual', 'channel', 'ram', 'compatibility', 'question']\n",
      "After stop words removal: ['dual', 'channel', 'ram', 'compatibility', 'question']\n",
      "After stemming with porters algorithm: ['dual', 'channel', 'ram', 'compat', 'quest']\n",
      "Tokenized sentence: ['adding', 'second', 'ram', 'stick', 'requirements']\n",
      "After stop words removal: ['adding', 'second', 'ram', 'stick', 'requirements']\n",
      "add\n",
      "After stemming with porters algorithm: ['ad', 'second', 'ram', 'stick', 'requir']\n",
      "Tokenized sentence: ['memory', 'matching', 'for', 'dual', 'channel']\n",
      "After stop words removal: ['memory', 'matching', 'dual', 'channel']\n",
      "match\n",
      "After stemming with porters algorithm: ['memori', 'matc', 'dual', 'channel']\n",
      "Tokenized sentence: ['different', 'ram', 'models', 'in', 'dual', 'channel']\n",
      "After stop words removal: ['different', 'ram', 'models', 'dual', 'channel']\n",
      "After stemming with porters algorithm: ['differ', 'ram', 'model', 'dual', 'channel']\n",
      "Tokenized sentence: ['ram', 'pairing', 'for', 'dual', 'channel', 'mode']\n",
      "After stop words removal: ['ram', 'pairing', 'dual', 'channel', 'mode']\n",
      "pair\n",
      "After stemming with porters algorithm: ['ram', 'pair', 'dual', 'channel', 'mode']\n",
      "Tokenized sentence: ['mixing', 'ram', 'brands', 'in', 'dual', 'channel']\n",
      "After stop words removal: ['mixing', 'ram', 'brands', 'dual', 'channel']\n",
      "mix\n",
      "After stemming with porters algorithm: ['mix', 'ram', 'brand', 'dual', 'channel']\n",
      "Tokenized sentence: ['dual', 'channel', 'ram', 'matching', 'requirements']\n",
      "After stop words removal: ['dual', 'channel', 'ram', 'matching', 'requirements']\n",
      "match\n",
      "After stemming with porters algorithm: ['dual', 'channel', 'ram', 'matc', 'requir']\n",
      "Tokenized sentence: ['cpu', 'high', 'temperature']\n",
      "After stop words removal: ['cpu', 'high', 'temperature']\n",
      "After stemming with porters algorithm: ['cpu', 'high', 'temperatur']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'cpu', 'overheating']\n",
      "After stop words removal: ['cpu', 'overheating']\n",
      "overheat\n",
      "overheate\n",
      "After stemming with porters algorithm: ['cpu', 'overh']\n",
      "Tokenized sentence: ['processor', 'running', 'too', 'hot']\n",
      "After stop words removal: ['processor', 'running', 'hot']\n",
      "runn\n",
      "After stemming with porters algorithm: ['processor', 'run', 'hot']\n",
      "Tokenized sentence: ['cpu', 'temperature', 'issues']\n",
      "After stop words removal: ['cpu', 'temperature', 'issues']\n",
      "After stemming with porters algorithm: ['cpu', 'temperatur', 'issu']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'cpu', 'overheating']\n",
      "After stop words removal: ['fix', 'cpu', 'overheating']\n",
      "overheat\n",
      "overheate\n",
      "After stemming with porters algorithm: ['fix', 'cpu', 'overh']\n",
      "Tokenized sentence: ['excessive', 'cpu', 'heat', 'problems']\n",
      "After stop words removal: ['excessive', 'cpu', 'heat', 'problems']\n",
      "After stemming with porters algorithm: ['excess', 'cpu', 'heat', 'problem']\n",
      "Tokenized sentence: ['processor', 'temperature', 'too', 'high']\n",
      "After stop words removal: ['processor', 'temperature', 'high']\n",
      "After stemming with porters algorithm: ['processor', 'temperatur', 'high']\n",
      "Tokenized sentence: ['cpu', 'thermal', 'throttling', 'issue']\n",
      "After stop words removal: ['cpu', 'thermal', 'throttling', 'issue']\n",
      "throttl\n",
      "After stemming with porters algorithm: ['cpu', 'thermal', 'throttl', 'issu']\n",
      "Tokenized sentence: ['ways', 'to', 'reduce', 'cpu', 'temperature']\n",
      "After stop words removal: ['ways', 'reduce', 'cpu', 'temperature']\n",
      "After stemming with porters algorithm: ['wai', 'reduc', 'cpu', 'temperatur']\n",
      "Tokenized sentence: ['cpu', 'cooling', 'problems']\n",
      "After stop words removal: ['cpu', 'cooling', 'problems']\n",
      "cool\n",
      "After stemming with porters algorithm: ['cpu', 'cool', 'problem']\n",
      "Tokenized sentence: ['your', 'computer', 'has', 'a', 'memory', 'problem', 'the', 'computer', 'has', 'rebooted', 'from', 'a', 'bugcheck']\n",
      "After stop words removal: ['computer', 'memory', 'problem', 'computer', 'rebooted', 'bugcheck']\n",
      "After stemming with porters algorithm: ['comput', 'memori', 'problem', 'comput', 'reboot', 'bugcheck']\n",
      "Tokenized sentence: ['memory', 'related', 'bugcheck', 'error']\n",
      "After stop words removal: ['memory', 'related', 'bugcheck', 'error']\n",
      "relate\n",
      "After stemming with porters algorithm: ['memori', 'relat', 'bugcheck', 'error']\n",
      "Tokenized sentence: ['computer', 'restarting', 'due', 'to', 'memory', 'problem']\n",
      "After stop words removal: ['computer', 'restarting', 'due', 'memory', 'problem']\n",
      "restart\n",
      "After stemming with porters algorithm: ['comput', 'restar', 'due', 'memori', 'problem']\n",
      "Tokenized sentence: ['ram', 'causing', 'system', 'bugcheck']\n",
      "After stop words removal: ['ram', 'causing', 'system', 'bugcheck']\n",
      "caus\n",
      "After stemming with porters algorithm: ['ram', 'caus', 'system', 'bugcheck']\n",
      "Tokenized sentence: ['memory', 'error', 'forcing', 'restart']\n",
      "After stop words removal: ['memory', 'error', 'forcing', 'restart']\n",
      "forc\n",
      "After stemming with porters algorithm: ['memori', 'error', 'for', 'restart']\n",
      "Tokenized sentence: ['system', 'memory', 'bugcheck', 'issue']\n",
      "After stop words removal: ['system', 'memory', 'bugcheck', 'issue']\n",
      "After stemming with porters algorithm: ['system', 'memori', 'bugcheck', 'issu']\n",
      "Tokenized sentence: ['ram', 'related', 'system', 'crashes']\n",
      "After stop words removal: ['ram', 'related', 'system', 'crashes']\n",
      "relate\n",
      "After stemming with porters algorithm: ['ram', 'relat', 'system', 'crash']\n",
      "Tokenized sentence: ['memory', 'bugcheck', 'troubleshooting']\n",
      "After stop words removal: ['memory', 'bugcheck', 'troubleshooting']\n",
      "troubleshoot\n",
      "After stemming with porters algorithm: ['memori', 'bugcheck', 'troubleshoot']\n",
      "Tokenized sentence: ['computer', 'memory', 'error', 'restart']\n",
      "After stop words removal: ['computer', 'memory', 'error', 'restart']\n",
      "After stemming with porters algorithm: ['comput', 'memori', 'error', 'restart']\n",
      "Tokenized sentence: ['memory', 'problem', 'causing', 'reboots']\n",
      "After stop words removal: ['memory', 'problem', 'causing', 'reboots']\n",
      "caus\n",
      "After stemming with porters algorithm: ['memori', 'problem', 'caus', 'reboot']\n",
      "Tokenized sentence: ['should', 'i', 'use', 'two', 'ram', 'sticks', 'instead', 'of', 'one']\n",
      "After stop words removal: ['use', 'two', 'ram', 'sticks', 'instead', 'one']\n",
      "After stemming with porters algorithm: ['us', 'two', 'ram', 'stick', 'instead', 'on']\n",
      "Tokenized sentence: ['is', 'there', 'a', 'benefit', 'to', 'using', 'two', 'ram', 'sticks']\n",
      "After stop words removal: ['benefit', 'using', 'two', 'ram', 'sticks']\n",
      "us\n",
      "After stemming with porters algorithm: ['benefit', 'us', 'two', 'ram', 'stick']\n",
      "Tokenized sentence: ['single', 'vs', 'dual', 'ram', 'stick', 'performance']\n",
      "After stop words removal: ['single', 'vs', 'dual', 'ram', 'stick', 'performance']\n",
      "After stemming with porters algorithm: ['singl', 'dual', 'ram', 'stick', 'perform']\n",
      "Tokenized sentence: ['benefits', 'of', 'using', 'two', 'ram', 'modules']\n",
      "After stop words removal: ['benefits', 'using', 'two', 'ram', 'modules']\n",
      "us\n",
      "After stemming with porters algorithm: ['benefit', 'us', 'two', 'ram', 'modul']\n",
      "Tokenized sentence: ['one', 'ram', 'stick', 'or', 'two', 'for', 'better', 'performance']\n",
      "After stop words removal: ['one', 'ram', 'stick', 'two', 'better', 'performance']\n",
      "After stemming with porters algorithm: ['on', 'ram', 'stick', 'two', 'better', 'perform']\n",
      "Tokenized sentence: ['dual', 'vs', 'single', 'ram', 'configuration']\n",
      "After stop words removal: ['dual', 'vs', 'single', 'ram', 'configuration']\n",
      "After stemming with porters algorithm: ['dual', 'singl', 'ram', 'configur']\n",
      "Tokenized sentence: ['advantages', 'of', 'using', 'two', 'ram', 'sticks']\n",
      "After stop words removal: ['advantages', 'using', 'two', 'ram', 'sticks']\n",
      "us\n",
      "After stemming with porters algorithm: ['advantag', 'us', 'two', 'ram', 'stick']\n",
      "Tokenized sentence: ['multiple', 'ram', 'sticks', 'vs', 'single', 'stick']\n",
      "After stop words removal: ['multiple', 'ram', 'sticks', 'vs', 'single', 'stick']\n",
      "After stemming with porters algorithm: ['multip', 'ram', 'stick', 'singl', 'stick']\n",
      "Tokenized sentence: ['performance', 'difference', 'between', 'one', 'and', 'two', 'ram', 'sticks']\n",
      "After stop words removal: ['performance', 'difference', 'one', 'two', 'ram', 'sticks']\n",
      "After stemming with porters algorithm: ['perform', 'differ', 'on', 'two', 'ram', 'stick']\n",
      "Tokenized sentence: ['why', 'use', 'two', 'ram', 'sticks', 'instead', 'of', 'one']\n",
      "After stop words removal: ['use', 'two', 'ram', 'sticks', 'instead', 'one']\n",
      "After stemming with porters algorithm: ['us', 'two', 'ram', 'stick', 'instead', 'on']\n",
      "Tokenized sentence: ['pc', 'freezes', 'mouse', 'and', 'keyboard', 'disconnect', 'randomly', 'until', 'force', 'restart']\n",
      "After stop words removal: ['pc', 'freezes', 'mouse', 'keyboard', 'disconnect', 'randomly', 'force', 'restart']\n",
      "After stemming with porters algorithm: ['freez', 'mous', 'keyboard', 'disconnect', 'randomli', 'forc', 'restart']\n",
      "Tokenized sentence: ['computer', 'freezing', 'with', 'peripherals', 'disconnecting']\n",
      "After stop words removal: ['computer', 'freezing', 'peripherals', 'disconnecting']\n",
      "freez\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['comput', 'freez', 'periph', 'disconnec']\n",
      "Tokenized sentence: ['system', 'freeze', 'with', 'usb', 'devices', 'disconnecting']\n",
      "After stop words removal: ['system', 'freeze', 'usb', 'devices', 'disconnecting']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['system', 'freez', 'usb', 'devic', 'disconnec']\n",
      "Tokenized sentence: ['random', 'pc', 'freeze', 'and', 'peripheral', 'failure']\n",
      "After stop words removal: ['random', 'pc', 'freeze', 'peripheral', 'failure']\n",
      "After stemming with porters algorithm: ['random', 'freez', 'periph', 'failur']\n",
      "Tokenized sentence: ['mouse', 'and', 'keyboard', 'disconnecting', 'during', 'freeze']\n",
      "After stop words removal: ['mouse', 'keyboard', 'disconnecting', 'freeze']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['mous', 'keyboard', 'disconnec', 'freez']\n",
      "Tokenized sentence: ['computer', 'locks', 'up', 'with', 'device', 'disconnects']\n",
      "After stop words removal: ['computer', 'locks', 'device', 'disconnects']\n",
      "After stemming with porters algorithm: ['comput', 'lock', 'devic', 'disconnect']\n",
      "Tokenized sentence: ['system', 'hangs', 'with', 'usb', 'disconnections']\n",
      "After stop words removal: ['system', 'hangs', 'usb', 'disconnections']\n",
      "After stemming with porters algorithm: ['system', 'hang', 'usb', 'disconnect']\n",
      "Tokenized sentence: ['pc', 'freezing', 'causing', 'peripheral', 'disconnects']\n",
      "After stop words removal: ['pc', 'freezing', 'causing', 'peripheral', 'disconnects']\n",
      "freez\n",
      "caus\n",
      "After stemming with porters algorithm: ['freez', 'caus', 'periph', 'disconnect']\n",
      "Tokenized sentence: ['random', 'freeze', 'with', 'input', 'device', 'failure']\n",
      "After stop words removal: ['random', 'freeze', 'input', 'device', 'failure']\n",
      "After stemming with porters algorithm: ['random', 'freez', 'input', 'devic', 'failur']\n",
      "Tokenized sentence: ['computer', 'freeze', 'forcing', 'restart']\n",
      "After stop words removal: ['computer', 'freeze', 'forcing', 'restart']\n",
      "forc\n",
      "After stemming with porters algorithm: ['comput', 'freez', 'for', 'restart']\n",
      "Tokenized sentence: ['new', 'ram', 'not', 'working']\n",
      "After stop words removal: ['new', 'ram', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['new', 'ram', 'wor']\n",
      "Tokenized sentence: ['ram', 'issues']\n",
      "After stop words removal: ['ram', 'issues']\n",
      "After stemming with porters algorithm: ['ram', 'issu']\n",
      "Tokenized sentence: ['fresh', 'ram', 'installation', 'problems']\n",
      "After stop words removal: ['fresh', 'ram', 'installation', 'problems']\n",
      "After stemming with porters algorithm: ['fresh', 'ram', 'instal', 'problem']\n",
      "Tokenized sentence: ['recently', 'installed', 'ram', 'not', 'functioning']\n",
      "After stop words removal: ['recently', 'installed', 'ram', 'functioning']\n",
      "function\n",
      "After stemming with porters algorithm: ['recent', 'instal', 'ram', 'funct']\n",
      "Tokenized sentence: ['new', 'memory', 'modules', 'not', 'working']\n",
      "After stop words removal: ['new', 'memory', 'modules', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['new', 'memori', 'modul', 'wor']\n",
      "Tokenized sentence: ['ram', 'installation', 'troubleshooting']\n",
      "After stop words removal: ['ram', 'installation', 'troubleshooting']\n",
      "troubleshoot\n",
      "After stemming with porters algorithm: ['ram', 'instal', 'troubleshoot']\n",
      "Tokenized sentence: ['new', 'ram', 'sticks', 'not', 'detected']\n",
      "After stop words removal: ['new', 'ram', 'sticks', 'detected']\n",
      "After stemming with porters algorithm: ['new', 'ram', 'stick', 'detec']\n",
      "Tokenized sentence: ['problem', 'with', 'new', 'ram', 'installation']\n",
      "After stop words removal: ['problem', 'new', 'ram', 'installation']\n",
      "After stemming with porters algorithm: ['problem', 'new', 'ram', 'instal']\n",
      "Tokenized sentence: ['recently', 'added', 'ram', 'not', 'working']\n",
      "After stop words removal: ['recently', 'added', 'ram', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['recent', 'ad', 'ram', 'wor']\n",
      "Tokenized sentence: ['new', 'memory', 'not', 'being', 'recognized']\n",
      "After stop words removal: ['new', 'memory', 'recognized']\n",
      "recognize\n",
      "After stemming with porters algorithm: ['new', 'memori', 'recogn']\n",
      "Tokenized sentence: ['pc', 'keeps', 'shutting', 'down', 'randomly']\n",
      "After stop words removal: ['pc', 'keeps', 'shutting', 'randomly']\n",
      "shutt\n",
      "After stemming with porters algorithm: ['keep', 'shut', 'randomli']\n",
      "Tokenized sentence: ['random', 'shutdowns']\n",
      "After stop words removal: ['random', 'shutdowns']\n",
      "After stemming with porters algorithm: ['random', 'shutdown']\n",
      "Tokenized sentence: ['computer', 'turns', 'off', 'unexpectedly']\n",
      "After stop words removal: ['computer', 'turns', 'unexpectedly']\n",
      "After stemming with porters algorithm: ['comput', 'turn', 'unexpectedli']\n",
      "Tokenized sentence: ['system', 'shutting', 'down', 'without', 'warning']\n",
      "After stop words removal: ['system', 'shutting', 'without', 'warning']\n",
      "shutt\n",
      "warn\n",
      "After stemming with porters algorithm: ['system', 'shut', 'without', 'war']\n",
      "Tokenized sentence: ['unexpected', 'pc', 'power', 'offs']\n",
      "After stop words removal: ['unexpected', 'pc', 'power', 'offs']\n",
      "After stemming with porters algorithm: ['unexpec', 'power', 'off']\n",
      "Tokenized sentence: ['random', 'computer', 'shutdowns']\n",
      "After stop words removal: ['random', 'computer', 'shutdowns']\n",
      "After stemming with porters algorithm: ['random', 'comput', 'shutdown']\n",
      "Tokenized sentence: ['pc', 'turning', 'off', 'by', 'itself']\n",
      "After stop words removal: ['pc', 'turning']\n",
      "turn\n",
      "After stemming with porters algorithm: ['tur']\n",
      "Tokenized sentence: ['system', 'powers', 'down', 'randomly']\n",
      "After stop words removal: ['system', 'powers', 'randomly']\n",
      "After stemming with porters algorithm: ['system', 'power', 'randomli']\n",
      "Tokenized sentence: ['unexpected', 'shutdown', 'problem']\n",
      "After stop words removal: ['unexpected', 'shutdown', 'problem']\n",
      "After stemming with porters algorithm: ['unexpec', 'shutdown', 'problem']\n",
      "Tokenized sentence: ['computer', 'randomly', 'powers', 'off']\n",
      "After stop words removal: ['computer', 'randomly', 'powers']\n",
      "After stemming with porters algorithm: ['comput', 'randomli', 'power']\n",
      "Tokenized sentence: ['psu', 'issue', 'after', 'upgrading', 'gpu']\n",
      "After stop words removal: ['psu', 'issue', 'upgrading', 'gpu']\n",
      "upgrad\n",
      "After stemming with porters algorithm: ['psu', 'issu', 'upgrad', 'gpu']\n",
      "Tokenized sentence: ['power', 'supply', 'problem', 'after', 'gpu', 'upgrade']\n",
      "After stop words removal: ['power', 'supply', 'problem', 'gpu', 'upgrade']\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'problem', 'gpu', 'upgrad']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'psu', 'struggling', 'after', 'upgrading', 'the', 'gpu']\n",
      "After stop words removal: ['psu', 'struggling', 'upgrading', 'gpu']\n",
      "struggl\n",
      "upgrad\n",
      "After stemming with porters algorithm: ['psu', 'struggl', 'upgrad', 'gpu']\n",
      "Tokenized sentence: ['does', 'upgrading', 'the', 'gpu', 'affect', 'the', 'psu']\n",
      "After stop words removal: ['upgrading', 'gpu', 'affect', 'psu']\n",
      "upgrad\n",
      "After stemming with porters algorithm: ['upgrad', 'gpu', 'affect', 'psu']\n",
      "Tokenized sentence: ['psu', 'not', 'handling', 'new', 'gpu']\n",
      "After stop words removal: ['psu', 'handling', 'new', 'gpu']\n",
      "handl\n",
      "After stemming with porters algorithm: ['psu', 'handl', 'new', 'gpu']\n",
      "Tokenized sentence: ['can', 'my', 'psu', 'support', 'the', 'new', 'gpu']\n",
      "After stop words removal: ['psu', 'support', 'new', 'gpu']\n",
      "After stemming with porters algorithm: ['psu', 'support', 'new', 'gpu']\n",
      "Tokenized sentence: ['gpu', 'upgrade', 'causing', 'power', 'issues']\n",
      "After stop words removal: ['gpu', 'upgrade', 'causing', 'power', 'issues']\n",
      "caus\n",
      "After stemming with porters algorithm: ['gpu', 'upgrad', 'caus', 'power', 'issu']\n",
      "Tokenized sentence: ['power', 'supply', 'failing', 'after', 'gpu', 'upgrade']\n",
      "After stop words removal: ['power', 'supply', 'failing', 'gpu', 'upgrade']\n",
      "fail\n",
      "After stemming with porters algorithm: ['power', 'suppli', 'fail', 'gpu', 'upgrad']\n",
      "Tokenized sentence: ['is', 'my', 'psu', 'enough', 'for', 'this', 'gpu']\n",
      "After stop words removal: ['psu', 'enough', 'gpu']\n",
      "After stemming with porters algorithm: ['psu', 'enough', 'gpu']\n",
      "Tokenized sentence: ['upgraded', 'gpu', 'causes', 'power', 'issues']\n",
      "After stop words removal: ['upgraded', 'gpu', 'causes', 'power', 'issues']\n",
      "After stemming with porters algorithm: ['upgrad', 'gpu', 'caus', 'power', 'issu']\n",
      "Tokenized sentence: ['computer', 'freezes', 'during', 'gaming']\n",
      "After stop words removal: ['computer', 'freezes', 'gaming']\n",
      "gam\n",
      "After stemming with porters algorithm: ['comput', 'freez', 'game']\n",
      "Tokenized sentence: ['game', 'freezes']\n",
      "After stop words removal: ['game', 'freezes']\n",
      "After stemming with porters algorithm: ['game', 'freez']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'game', 'freeze', 'randomly']\n",
      "After stop words removal: ['game', 'freeze', 'randomly']\n",
      "After stemming with porters algorithm: ['game', 'freez', 'randomli']\n",
      "Tokenized sentence: ['my', 'computer', 'freezes', 'in', 'games']\n",
      "After stop words removal: ['computer', 'freezes', 'games']\n",
      "After stemming with porters algorithm: ['comput', 'freez', 'game']\n",
      "Tokenized sentence: ['games', 'freeze', 'after', 'some', 'time']\n",
      "After stop words removal: ['games', 'freeze', 'time']\n",
      "After stemming with porters algorithm: ['game', 'freez', 'time']\n",
      "Tokenized sentence: ['what', 'causes', 'games', 'to', 'freeze']\n",
      "After stop words removal: ['causes', 'games', 'freeze']\n",
      "After stemming with porters algorithm: ['caus', 'game', 'freez']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'stop', 'my', 'game', 'from', 'freezing']\n",
      "After stop words removal: ['stop', 'game', 'freezing']\n",
      "freez\n",
      "After stemming with porters algorithm: ['stop', 'game', 'freez']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'pc', 'freeze', 'when', 'gaming']\n",
      "After stop words removal: ['pc', 'freeze', 'gaming']\n",
      "gam\n",
      "After stemming with porters algorithm: ['freez', 'game']\n",
      "Tokenized sentence: ['game', 'crashes', 'when', 'playing']\n",
      "After stop words removal: ['game', 'crashes', 'playing']\n",
      "play\n",
      "After stemming with porters algorithm: ['game', 'crash', 'plai']\n",
      "Tokenized sentence: ['freezing', 'issues', 'during', 'gaming']\n",
      "After stop words removal: ['freezing', 'issues', 'gaming']\n",
      "freez\n",
      "gam\n",
      "After stemming with porters algorithm: ['freez', 'issu', 'game']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'internet', 'so', 'slow']\n",
      "After stop words removal: ['internet', 'slow']\n",
      "After stemming with porters algorithm: ['internet', 'slow']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'increase', 'my', 'internet', 'speed']\n",
      "After stop words removal: ['increase', 'internet', 'speed']\n",
      "After stemming with porters algorithm: ['increas', 'internet', 'speed']\n",
      "Tokenized sentence: ['my', 'internet', 'is', 'sluggish', 'what', 'can', 'i', 'do']\n",
      "After stop words removal: ['internet', 'sluggish']\n",
      "After stemming with porters algorithm: ['internet', 'sluggish']\n",
      "Tokenized sentence: ['slow', 'internet', 'connection', 'issues']\n",
      "After stop words removal: ['slow', 'internet', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['slow', 'internet', 'connect', 'issu']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'improve', 'internet', 'performance']\n",
      "After stop words removal: ['improve', 'internet', 'performance']\n",
      "After stemming with porters algorithm: ['improv', 'internet', 'perform']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'wifi', 'speed', 'so', 'low']\n",
      "After stop words removal: ['wifi', 'speed', 'low']\n",
      "After stemming with porters algorithm: ['wifi', 'speed', 'low']\n",
      "Tokenized sentence: ['what', 'can', 'i', 'do', 'to', 'boost', 'my', 'internet', 'speed']\n",
      "After stop words removal: ['boost', 'internet', 'speed']\n",
      "After stemming with porters algorithm: ['boost', 'internet', 'speed']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'broadband', 'so', 'slow']\n",
      "After stop words removal: ['broadband', 'slow']\n",
      "After stemming with porters algorithm: ['broadband', 'slow']\n",
      "Tokenized sentence: ['how', 'to', 'speed', 'up', 'a', 'slow', 'internet', 'connection']\n",
      "After stop words removal: ['speed', 'slow', 'internet', 'connection']\n",
      "After stemming with porters algorithm: ['speed', 'slow', 'internet', 'connect']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'a', 'slow', 'internet', 'connection']\n",
      "After stop words removal: ['fix', 'slow', 'internet', 'connection']\n",
      "After stemming with porters algorithm: ['fix', 'slow', 'internet', 'connect']\n",
      "Tokenized sentence: ['ethernet', 'not', 'working']\n",
      "After stop words removal: ['ethernet', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['ethernet', 'wor']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'connect', 'through', 'ethernet']\n",
      "After stop words removal: ['connect', 'ethernet']\n",
      "After stemming with porters algorithm: ['connect', 'ethernet']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'ethernet', 'connection', 'problems']\n",
      "After stop words removal: ['fix', 'ethernet', 'connection', 'problems']\n",
      "After stemming with porters algorithm: ['fix', 'ethernet', 'connect', 'problem']\n",
      "Tokenized sentence: ['ethernet', 'connection', 'is', 'unstable']\n",
      "After stop words removal: ['ethernet', 'connection', 'unstable']\n",
      "After stemming with porters algorithm: ['ethernet', 'connect', 'unstab']\n",
      "Tokenized sentence: ['why', 'did', 'my', 'ethernet', 'connection', 'drop']\n",
      "After stop words removal: ['ethernet', 'connection', 'drop']\n",
      "After stemming with porters algorithm: ['ethernet', 'connect', 'drop']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'ethernet', 'not', 'recognized']\n",
      "After stop words removal: ['ethernet', 'recognized']\n",
      "recognize\n",
      "After stemming with porters algorithm: ['ethernet', 'recogn']\n",
      "Tokenized sentence: ['ethernet', 'port', 'not', 'working']\n",
      "After stop words removal: ['ethernet', 'port', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['ethernet', 'port', 'wor']\n",
      "Tokenized sentence: ['ethernet', 'issues', 'on', 'windows']\n",
      "After stop words removal: ['ethernet', 'issues', 'windows']\n",
      "After stemming with porters algorithm: ['ethernet', 'issu', 'window']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'ethernet', 'not', 'connecting']\n",
      "After stop words removal: ['ethernet', 'connecting']\n",
      "connect\n",
      "After stemming with porters algorithm: ['ethernet', 'connec']\n",
      "Tokenized sentence: ['ethernet', 'connection', 'not', 'available']\n",
      "After stop words removal: ['ethernet', 'connection', 'available']\n",
      "After stemming with porters algorithm: ['ethernet', 'connect', 'avail']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reset', 'my', 'network', 'settings']\n",
      "After stop words removal: ['reset', 'network', 'settings']\n",
      "sett\n",
      "After stemming with porters algorithm: ['reset', 'network', 'set']\n",
      "Tokenized sentence: ['reset', 'network', 'settings', 'in', 'windows']\n",
      "After stop words removal: ['reset', 'network', 'settings', 'windows']\n",
      "sett\n",
      "After stemming with porters algorithm: ['reset', 'network', 'set', 'window']\n",
      "Tokenized sentence: ['i', 'need', 'to', 'reset', 'my', 'network', 'configuration']\n",
      "After stop words removal: ['need', 'reset', 'network', 'configuration']\n",
      "After stemming with porters algorithm: ['need', 'reset', 'network', 'configur']\n",
      "Tokenized sentence: ['network', 'reset', 'not', 'working']\n",
      "After stop words removal: ['network', 'reset', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['network', 'reset', 'wor']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'network', 'reset', 'issues']\n",
      "After stop words removal: ['fix', 'network', 'reset', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'network', 'reset', 'issu']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'perform', 'a', 'network', 'reset']\n",
      "After stop words removal: ['perform', 'network', 'reset']\n",
      "After stemming with porters algorithm: ['perform', 'network', 'reset']\n",
      "Tokenized sentence: ['why', 'isn', 't', 'my', 'network', 'reset', 'working']\n",
      "After stop words removal: ['network', 'reset', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['network', 'reset', 'wor']\n",
      "Tokenized sentence: ['how', 'to', 'reset', 'network', 'settings', 'in', 'windows']\n",
      "After stop words removal: ['reset', 'network', 'settings', 'windows']\n",
      "sett\n",
      "After stemming with porters algorithm: ['reset', 'network', 'set', 'window']\n",
      "Tokenized sentence: ['network', 'reset', 'not', 'fixing', 'problems']\n",
      "After stop words removal: ['network', 'reset', 'fixing', 'problems']\n",
      "fix\n",
      "After stemming with porters algorithm: ['network', 'reset', 'fix', 'problem']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reconfigure', 'my', 'network', 'after', 'reset']\n",
      "After stop words removal: ['reconfigure', 'network', 'reset']\n",
      "After stemming with porters algorithm: ['reconfigur', 'network', 'reset']\n",
      "Tokenized sentence: ['surface', 'can', 't', 'connect', 'to', 'wifi']\n",
      "After stop words removal: ['surface', 'connect', 'wifi']\n",
      "After stemming with porters algorithm: ['surfac', 'connect', 'wifi']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'surface', 'not', 'connecting']\n",
      "After stop words removal: ['surface', 'connecting']\n",
      "connect\n",
      "After stemming with porters algorithm: ['surfac', 'connec']\n",
      "Tokenized sentence: ['wifi', 'issues', 'on', 'surface']\n",
      "After stop words removal: ['wifi', 'issues', 'surface']\n",
      "After stemming with porters algorithm: ['wifi', 'issu', 'surfac']\n",
      "Tokenized sentence: ['surface', 'wifi', 'problems']\n",
      "After stop words removal: ['surface', 'wifi', 'problems']\n",
      "After stemming with porters algorithm: ['surfac', 'wifi', 'problem']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'wifi', 'on', 'my', 'surface']\n",
      "After stop words removal: ['fix', 'wifi', 'surface']\n",
      "After stemming with porters algorithm: ['fix', 'wifi', 'surfac']\n",
      "Tokenized sentence: ['surface', 'wifi', 'not', 'working']\n",
      "After stop words removal: ['surface', 'wifi', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['surfac', 'wifi', 'wor']\n",
      "Tokenized sentence: ['surface', 'can', 't', 'detect', 'wifi', 'networks']\n",
      "After stop words removal: ['surface', 'detect', 'wifi', 'networks']\n",
      "After stemming with porters algorithm: ['surfac', 'detect', 'wifi', 'network']\n",
      "Tokenized sentence: ['surface', 'wifi', 'connection', 'issue']\n",
      "After stop words removal: ['surface', 'wifi', 'connection', 'issue']\n",
      "After stemming with porters algorithm: ['surfac', 'wifi', 'connect', 'issu']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'surface', 'keep', 'disconnecting', 'from', 'wifi']\n",
      "After stop words removal: ['surface', 'keep', 'disconnecting', 'wifi']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['surfac', 'keep', 'disconnec', 'wifi']\n",
      "Tokenized sentence: ['how', 'to', 'resolve', 'surface', 'wifi', 'issues']\n",
      "After stop words removal: ['resolve', 'surface', 'wifi', 'issues']\n",
      "After stemming with porters algorithm: ['resolv', 'surfac', 'wifi', 'issu']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'wifi', 'signal', 'weak']\n",
      "After stop words removal: ['wifi', 'signal', 'weak']\n",
      "After stemming with porters algorithm: ['wifi', 'signal', 'weak']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'improve', 'wifi', 'signal', 'strength']\n",
      "After stop words removal: ['improve', 'wifi', 'signal', 'strength']\n",
      "After stemming with porters algorithm: ['improv', 'wifi', 'signal', 'strength']\n",
      "Tokenized sentence: ['weak', 'wifi', 'connection', 'at', 'home']\n",
      "After stop words removal: ['weak', 'wifi', 'connection', 'home']\n",
      "After stemming with porters algorithm: ['weak', 'wifi', 'connect', 'home']\n",
      "Tokenized sentence: ['my', 'wifi', 'drops', 'when', 'i', 'm', 'far', 'from', 'the', 'router']\n",
      "After stop words removal: ['wifi', 'drops', 'far', 'router']\n",
      "After stemming with porters algorithm: ['wifi', 'drop', 'far', 'router']\n",
      "Tokenized sentence: ['what', 'causes', 'weak', 'wifi', 'signals']\n",
      "After stop words removal: ['causes', 'weak', 'wifi', 'signals']\n",
      "After stemming with porters algorithm: ['caus', 'weak', 'wifi', 'signal']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'boost', 'my', 'wifi', 'signal']\n",
      "After stop words removal: ['boost', 'wifi', 'signal']\n",
      "After stemming with porters algorithm: ['boost', 'wifi', 'signal']\n",
      "Tokenized sentence: ['how', 'to', 'get', 'better', 'wifi', 'reception']\n",
      "After stop words removal: ['get', 'better', 'wifi', 'reception']\n",
      "After stemming with porters algorithm: ['get', 'better', 'wifi', 'recept']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'wifi', 'signal', 'drop']\n",
      "After stop words removal: ['wifi', 'signal', 'drop']\n",
      "After stemming with porters algorithm: ['wifi', 'signal', 'drop']\n",
      "Tokenized sentence: ['how', 'to', 'increase', 'wifi', 'signal', 'range']\n",
      "After stop words removal: ['increase', 'wifi', 'signal', 'range']\n",
      "After stemming with porters algorithm: ['increas', 'wifi', 'signal', 'rang']\n",
      "Tokenized sentence: ['what', 'can', 'i', 'do', 'about', 'poor', 'wifi', 'signal']\n",
      "After stop words removal: ['poor', 'wifi', 'signal']\n",
      "After stemming with porters algorithm: ['poor', 'wifi', 'signal']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reset', 'my', 'router']\n",
      "After stop words removal: ['reset', 'router']\n",
      "After stemming with porters algorithm: ['reset', 'router']\n",
      "Tokenized sentence: ['router', 'reset', 'instructions']\n",
      "After stop words removal: ['router', 'reset', 'instructions']\n",
      "After stemming with porters algorithm: ['router', 'reset', 'instruct']\n",
      "Tokenized sentence: ['why', 'should', 'i', 'reset', 'my', 'router']\n",
      "After stop words removal: ['reset', 'router']\n",
      "After stemming with porters algorithm: ['reset', 'router']\n",
      "Tokenized sentence: ['will', 'resetting', 'my', 'router', 'fix', 'connection', 'issues']\n",
      "After stop words removal: ['resetting', 'router', 'fix', 'connection', 'issues']\n",
      "resett\n",
      "After stemming with porters algorithm: ['reset', 'router', 'fix', 'connect', 'issu']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'restart', 'my', 'modem', 'and', 'router']\n",
      "After stop words removal: ['restart', 'modem', 'router']\n",
      "After stemming with porters algorithm: ['restart', 'modem', 'router']\n",
      "Tokenized sentence: ['what', 'happens', 'when', 'i', 'reset', 'my', 'router']\n",
      "After stop words removal: ['happens', 'reset', 'router']\n",
      "After stemming with porters algorithm: ['happen', 'reset', 'router']\n",
      "Tokenized sentence: ['why', 'do', 'i', 'need', 'to', 'reset', 'my', 'router']\n",
      "After stop words removal: ['need', 'reset', 'router']\n",
      "After stemming with porters algorithm: ['need', 'reset', 'router']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reset', 'my', 'home', 'router']\n",
      "After stop words removal: ['reset', 'home', 'router']\n",
      "After stemming with porters algorithm: ['reset', 'home', 'router']\n",
      "Tokenized sentence: ['router', 'reset', 'to', 'fix', 'internet', 'problems']\n",
      "After stop words removal: ['router', 'reset', 'fix', 'internet', 'problems']\n",
      "After stemming with porters algorithm: ['router', 'reset', 'fix', 'internet', 'problem']\n",
      "Tokenized sentence: ['how', 'to', 'perform', 'a', 'router', 'reset']\n",
      "After stop words removal: ['perform', 'router', 'reset']\n",
      "After stemming with porters algorithm: ['perform', 'router', 'reset']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'connect', 'to', 'my', 'vpn']\n",
      "After stop words removal: ['connect', 'vpn']\n",
      "After stemming with porters algorithm: ['connect', 'vpn']\n",
      "Tokenized sentence: ['vpn', 'not', 'working', 'on', 'windows']\n",
      "After stop words removal: ['vpn', 'working', 'windows']\n",
      "work\n",
      "After stemming with porters algorithm: ['vpn', 'wor', 'window']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'vpn', 'connection', 'issues']\n",
      "After stop words removal: ['fix', 'vpn', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'vpn', 'connect', 'issu']\n",
      "Tokenized sentence: ['vpn', 'connection', 'is', 'slow']\n",
      "After stop words removal: ['vpn', 'connection', 'slow']\n",
      "After stemming with porters algorithm: ['vpn', 'connect', 'slow']\n",
      "Tokenized sentence: ['vpn', 'keeps', 'disconnecting']\n",
      "After stop words removal: ['vpn', 'keeps', 'disconnecting']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['vpn', 'keep', 'disconnec']\n",
      "Tokenized sentence: ['why', 'won', 't', 'my', 'vpn', 'connect']\n",
      "After stop words removal: ['vpn', 'connect']\n",
      "After stemming with porters algorithm: ['vpn', 'connect']\n",
      "Tokenized sentence: ['how', 'to', 'resolve', 'vpn', 'connection', 'issues']\n",
      "After stop words removal: ['resolve', 'vpn', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['resolv', 'vpn', 'connect', 'issu']\n",
      "Tokenized sentence: ['vpn', 'not', 'responding']\n",
      "After stop words removal: ['vpn', 'responding']\n",
      "respond\n",
      "After stemming with porters algorithm: ['vpn', 'respon']\n",
      "Tokenized sentence: ['vpn', 'connection', 'dropping']\n",
      "After stop words removal: ['vpn', 'connection', 'dropping']\n",
      "dropp\n",
      "After stemming with porters algorithm: ['vpn', 'connect', 'drop']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'slow', 'vpn', 'speeds']\n",
      "After stop words removal: ['fix', 'slow', 'vpn', 'speeds']\n",
      "After stemming with porters algorithm: ['fix', 'slow', 'vpn', 'speed']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'firewall', 'blocking', 'the', 'connection']\n",
      "After stop words removal: ['firewall', 'blocking', 'connection']\n",
      "block\n",
      "After stemming with porters algorithm: ['firewal', 'bloc', 'connect']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'firewall', 'blocking', 'issues']\n",
      "After stop words removal: ['fix', 'firewall', 'blocking', 'issues']\n",
      "block\n",
      "After stemming with porters algorithm: ['fix', 'firewal', 'bloc', 'issu']\n",
      "Tokenized sentence: ['my', 'firewall', 'is', 'causing', 'network', 'problems']\n",
      "After stop words removal: ['firewall', 'causing', 'network', 'problems']\n",
      "caus\n",
      "After stemming with porters algorithm: ['firewal', 'caus', 'network', 'problem']\n",
      "Tokenized sentence: ['firewall', 'settings', 'prevent', 'me', 'from', 'connecting']\n",
      "After stop words removal: ['firewall', 'settings', 'prevent', 'connecting']\n",
      "sett\n",
      "connect\n",
      "After stemming with porters algorithm: ['firewal', 'set', 'prevent', 'connec']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'configure', 'my', 'firewall', 'for', 'network', 'access']\n",
      "After stop words removal: ['configure', 'firewall', 'network', 'access']\n",
      "After stemming with porters algorithm: ['configur', 'firewal', 'network', 'access']\n",
      "Tokenized sentence: ['firewall', 'keeps', 'blocking', 'certain', 'websites']\n",
      "After stop words removal: ['firewall', 'keeps', 'blocking', 'certain', 'websites']\n",
      "block\n",
      "After stemming with porters algorithm: ['firewal', 'keep', 'bloc', 'certain', 'websit']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'allow', 'specific', 'apps', 'through', 'the', 'firewall']\n",
      "After stop words removal: ['allow', 'specific', 'apps', 'firewall']\n",
      "After stemming with porters algorithm: ['allow', 'specif', 'app', 'firewal']\n",
      "Tokenized sentence: ['what', 'is', 'the', 'cause', 'of', 'firewall', 'connection', 'problems']\n",
      "After stop words removal: ['cause', 'firewall', 'connection', 'problems']\n",
      "After stemming with porters algorithm: ['caus', 'firewal', 'connect', 'problem']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'access', 'the', 'internet', 'because', 'of', 'my', 'firewall']\n",
      "After stop words removal: ['access', 'internet', 'firewall']\n",
      "After stemming with porters algorithm: ['access', 'internet', 'firewal']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'disable', 'my', 'firewall', 'to', 'test', 'network', 'issues']\n",
      "After stop words removal: ['disable', 'firewall', 'test', 'network', 'issues']\n",
      "After stemming with porters algorithm: ['disab', 'firewal', 'test', 'network', 'issu']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'modem', 'not', 'working']\n",
      "After stop words removal: ['modem', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['modem', 'wor']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'troubleshoot', 'modem', 'problems']\n",
      "After stop words removal: ['troubleshoot', 'modem', 'problems']\n",
      "After stemming with porters algorithm: ['troubleshoot', 'modem', 'problem']\n",
      "Tokenized sentence: ['my', 'modem', 'is', 'not', 'connecting', 'to', 'the', 'internet']\n",
      "After stop words removal: ['modem', 'connecting', 'internet']\n",
      "connect\n",
      "After stemming with porters algorithm: ['modem', 'connec', 'internet']\n",
      "Tokenized sentence: ['modem', 'issues', 'in', 'windows']\n",
      "After stop words removal: ['modem', 'issues', 'windows']\n",
      "After stemming with porters algorithm: ['modem', 'issu', 'window']\n",
      "Tokenized sentence: ['how', 'to', 'reset', 'a', 'modem']\n",
      "After stop words removal: ['reset', 'modem']\n",
      "After stemming with porters algorithm: ['reset', 'modem']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'modem', 'blinking', 'red']\n",
      "After stop words removal: ['modem', 'blinking', 'red']\n",
      "blink\n",
      "After stemming with porters algorithm: ['modem', 'blin', 'red']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'check', 'if', 'my', 'modem', 'is', 'faulty']\n",
      "After stop words removal: ['check', 'modem', 'faulty']\n",
      "After stemming with porters algorithm: ['check', 'modem', 'faulti']\n",
      "Tokenized sentence: ['what', 'should', 'i', 'do', 'if', 'my', 'modem', 'is', 'not', 'getting', 'a', 'signal']\n",
      "After stop words removal: ['modem', 'getting', 'signal']\n",
      "gett\n",
      "After stemming with porters algorithm: ['modem', 'get', 'signal']\n",
      "Tokenized sentence: ['modem', 'not', 'syncing', 'with', 'the', 'network']\n",
      "After stop words removal: ['modem', 'syncing', 'network']\n",
      "After stemming with porters algorithm: ['modem', 'syncing', 'network']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'fix', 'modem', 'connectivity', 'problems']\n",
      "After stop words removal: ['fix', 'modem', 'connectivity', 'problems']\n",
      "After stemming with porters algorithm: ['fix', 'modem', 'connect', 'problem']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'configure', 'a', 'vpn']\n",
      "After stop words removal: ['configure', 'vpn']\n",
      "After stemming with porters algorithm: ['configur', 'vpn']\n",
      "Tokenized sentence: ['setting', 'up', 'a', 'vpn', 'on', 'windows']\n",
      "After stop words removal: ['setting', 'vpn', 'windows']\n",
      "sett\n",
      "After stemming with porters algorithm: ['set', 'vpn', 'window']\n",
      "Tokenized sentence: ['how', 'to', 'configure', 'vpn', 'settings']\n",
      "After stop words removal: ['configure', 'vpn', 'settings']\n",
      "sett\n",
      "After stemming with porters algorithm: ['configur', 'vpn', 'set']\n",
      "Tokenized sentence: ['vpn', 'configuration', 'for', 'remote', 'work']\n",
      "After stop words removal: ['vpn', 'configuration', 'remote', 'work']\n",
      "After stemming with porters algorithm: ['vpn', 'configur', 'remot', 'work']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'set', 'up', 'a', 'vpn', 'connection']\n",
      "After stop words removal: ['set', 'vpn', 'connection']\n",
      "After stemming with porters algorithm: ['set', 'vpn', 'connect']\n",
      "Tokenized sentence: ['vpn', 'not', 'connecting', 'after', 'configuration']\n",
      "After stop words removal: ['vpn', 'connecting', 'configuration']\n",
      "connect\n",
      "After stemming with porters algorithm: ['vpn', 'connec', 'configur']\n",
      "Tokenized sentence: ['which', 'vpn', 'protocol', 'should', 'i', 'use']\n",
      "After stop words removal: ['vpn', 'protocol', 'use']\n",
      "After stemming with porters algorithm: ['vpn', 'protocol', 'us']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'configure', 'a', 'vpn', 'on', 'my', 'router']\n",
      "After stop words removal: ['configure', 'vpn', 'router']\n",
      "After stemming with porters algorithm: ['configur', 'vpn', 'router']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'set', 'up', 'a', 'vpn', 'for', 'my', 'workplace']\n",
      "After stop words removal: ['set', 'vpn', 'workplace']\n",
      "After stemming with porters algorithm: ['set', 'vpn', 'workplac']\n",
      "Tokenized sentence: ['what', 'is', 'the', 'best', 'vpn', 'configuration', 'for', 'speed']\n",
      "After stop words removal: ['best', 'vpn', 'configuration', 'speed']\n",
      "After stemming with porters algorithm: ['best', 'vpn', 'configur', 'speed']\n",
      "Tokenized sentence: ['wifi', 'losing', 'signal']\n",
      "After stop words removal: ['wifi', 'losing', 'signal']\n",
      "los\n",
      "After stemming with porters algorithm: ['wifi', 'lose', 'signal']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'wifi', 'signal', 'drops']\n",
      "After stop words removal: ['fix', 'wifi', 'signal', 'drops']\n",
      "After stemming with porters algorithm: ['fix', 'wifi', 'signal', 'drop']\n",
      "Tokenized sentence: ['wifi', 'interference', 'issues']\n",
      "After stop words removal: ['wifi', 'interference', 'issues']\n",
      "After stemming with porters algorithm: ['wifi', 'interf', 'issu']\n",
      "Tokenized sentence: ['wifi', 'keeps', 'disconnecting', 'due', 'to', 'signal']\n",
      "After stop words removal: ['wifi', 'keeps', 'disconnecting', 'due', 'signal']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['wifi', 'keep', 'disconnec', 'due', 'signal']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'wifi', 'lose', 'signal']\n",
      "After stop words removal: ['wifi', 'lose', 'signal']\n",
      "After stemming with porters algorithm: ['wifi', 'lose', 'signal']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'wifi', 'congestion']\n",
      "After stop words removal: ['fix', 'wifi', 'congestion']\n",
      "After stemming with porters algorithm: ['fix', 'wifi', 'congest']\n",
      "Tokenized sentence: ['what', 'are', 'the', 'best', 'wifi', 'channels', 'for', 'my', 'router']\n",
      "After stop words removal: ['best', 'wifi', 'channels', 'router']\n",
      "After stemming with porters algorithm: ['best', 'wifi', 'channel', 'router']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'stop', 'wifi', 'interference', 'from', 'other', 'networks']\n",
      "After stop words removal: ['stop', 'wifi', 'interference', 'networks']\n",
      "After stemming with porters algorithm: ['stop', 'wifi', 'interf', 'network']\n",
      "Tokenized sentence: ['my', 'wifi', 'signal', 'drops', 'when', 'i', 'm', 'far', 'from', 'the', 'router']\n",
      "After stop words removal: ['wifi', 'signal', 'drops', 'far', 'router']\n",
      "After stemming with porters algorithm: ['wifi', 'signal', 'drop', 'far', 'router']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'wifi', 'have', 'intermittent', 'disconnections']\n",
      "After stop words removal: ['wifi', 'intermittent', 'disconnections']\n",
      "After stemming with porters algorithm: ['wifi', 'intermitt', 'disconnect']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'connect', 'to', 'my', 'network', 'printer']\n",
      "After stop words removal: ['connect', 'network', 'printer']\n",
      "After stemming with porters algorithm: ['connect', 'network', 'printer']\n",
      "Tokenized sentence: ['printer', 'not', 'connecting', 'to', 'wifi']\n",
      "After stop words removal: ['printer', 'connecting', 'wifi']\n",
      "connect\n",
      "After stemming with porters algorithm: ['printer', 'connec', 'wifi']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'network', 'printer', 'issues']\n",
      "After stop words removal: ['fix', 'network', 'printer', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'network', 'printer', 'issu']\n",
      "Tokenized sentence: ['printer', 'connection', 'timed', 'out']\n",
      "After stop words removal: ['printer', 'connection', 'timed']\n",
      "After stemming with porters algorithm: ['printer', 'connect', 'time']\n",
      "Tokenized sentence: ['printer', 'not', 'found', 'on', 'the', 'network']\n",
      "After stop words removal: ['printer', 'found', 'network']\n",
      "After stemming with porters algorithm: ['printer', 'found', 'network']\n",
      "Tokenized sentence: ['why', 'can', 't', 'my', 'printer', 'find', 'the', 'network']\n",
      "After stop words removal: ['printer', 'find', 'network']\n",
      "After stemming with porters algorithm: ['printer', 'find', 'network']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'reconfigure', 'my', 'network', 'printer']\n",
      "After stop words removal: ['reconfigure', 'network', 'printer']\n",
      "After stemming with porters algorithm: ['reconfigur', 'network', 'printer']\n",
      "Tokenized sentence: ['my', 'network', 'printer', 'is', 'offline']\n",
      "After stop words removal: ['network', 'printer', 'offline']\n",
      "After stemming with porters algorithm: ['network', 'printer', 'offlin']\n",
      "Tokenized sentence: ['printer', 'says', 'not', 'connected', 'to', 'network']\n",
      "After stop words removal: ['printer', 'says', 'connected', 'network']\n",
      "After stemming with porters algorithm: ['printer', 'sai', 'connec', 'network']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'printer', 'having', 'trouble', 'with', 'wifi']\n",
      "After stop words removal: ['printer', 'trouble', 'wifi']\n",
      "After stemming with porters algorithm: ['printer', 'troubl', 'wifi']\n",
      "Tokenized sentence: ['router', 'firmware', 'update', 'failed']\n",
      "After stop words removal: ['router', 'firmware', 'update', 'failed']\n",
      "After stemming with porters algorithm: ['router', 'firmwar', 'updat', 'fail']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'update', 'my', 'router', 'firmware']\n",
      "After stop words removal: ['update', 'router', 'firmware']\n",
      "After stemming with porters algorithm: ['updat', 'router', 'firmwar']\n",
      "Tokenized sentence: ['firmware', 'update', 'problem']\n",
      "After stop words removal: ['firmware', 'update', 'problem']\n",
      "After stemming with porters algorithm: ['firmwar', 'updat', 'problem']\n",
      "Tokenized sentence: ['how', 'to', 'fix', 'router', 'firmware', 'issues']\n",
      "After stop words removal: ['fix', 'router', 'firmware', 'issues']\n",
      "After stemming with porters algorithm: ['fix', 'router', 'firmwar', 'issu']\n",
      "Tokenized sentence: ['firmware', 'update', 'failed', 'on', 'my', 'router']\n",
      "After stop words removal: ['firmware', 'update', 'failed', 'router']\n",
      "After stemming with porters algorithm: ['firmwar', 'updat', 'fail', 'router']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'restore', 'router', 'firmware']\n",
      "After stop words removal: ['restore', 'router', 'firmware']\n",
      "After stemming with porters algorithm: ['restor', 'router', 'firmwar']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'router', 'firmware', 'update', 'not', 'completing']\n",
      "After stop words removal: ['router', 'firmware', 'update', 'completing']\n",
      "complet\n",
      "After stemming with porters algorithm: ['router', 'firmwar', 'updat', 'complet']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'install', 'router', 'firmware', 'manually']\n",
      "After stop words removal: ['install', 'router', 'firmware', 'manually']\n",
      "After stemming with porters algorithm: ['instal', 'router', 'firmwar', 'manual']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'router', 'fail', 'after', 'a', 'firmware', 'update']\n",
      "After stop words removal: ['router', 'fail', 'firmware', 'update']\n",
      "After stemming with porters algorithm: ['router', 'fail', 'firmwar', 'updat']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'troubleshoot', 'firmware', 'update', 'failures']\n",
      "After stop words removal: ['troubleshoot', 'firmware', 'update', 'failures']\n",
      "After stemming with porters algorithm: ['troubleshoot', 'firmwar', 'updat', 'failur']\n",
      "Tokenized sentence: ['wifi', 'disconnects', 'after', 'sleep', 'mode']\n",
      "After stop words removal: ['wifi', 'disconnects', 'sleep', 'mode']\n",
      "After stemming with porters algorithm: ['wifi', 'disconnect', 'sleep', 'mode']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'wifi', 'disconnect', 'when', 'my', 'phone', 'is', 'asleep']\n",
      "After stop words removal: ['wifi', 'disconnect', 'phone', 'asleep']\n",
      "After stemming with porters algorithm: ['wifi', 'disconnect', 'phone', 'asleep']\n",
      "Tokenized sentence: ['how', 'to', 'keep', 'wifi', 'connected', 'during', 'sleep']\n",
      "After stop words removal: ['keep', 'wifi', 'connected', 'sleep']\n",
      "After stemming with porters algorithm: ['keep', 'wifi', 'connec', 'sleep']\n",
      "Tokenized sentence: ['wifi', 'keeps', 'turning', 'off', 'after', 'sleep']\n",
      "After stop words removal: ['wifi', 'keeps', 'turning', 'sleep']\n",
      "turn\n",
      "After stemming with porters algorithm: ['wifi', 'keep', 'tur', 'sleep']\n",
      "Tokenized sentence: ['wifi', 'disconnects', 'when', 'phone', 'is', 'locked']\n",
      "After stop words removal: ['wifi', 'disconnects', 'phone', 'locked']\n",
      "After stemming with porters algorithm: ['wifi', 'disconnect', 'phone', 'loc']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'wifi', 'drop', 'after', 'standby', 'mode']\n",
      "After stop words removal: ['wifi', 'drop', 'standby', 'mode']\n",
      "After stemming with porters algorithm: ['wifi', 'drop', 'standbi', 'mode']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'prevent', 'wifi', 'disconnections', 'after', 'sleep']\n",
      "After stop words removal: ['prevent', 'wifi', 'disconnections', 'sleep']\n",
      "After stemming with porters algorithm: ['prevent', 'wifi', 'disconnect', 'sleep']\n",
      "Tokenized sentence: ['how', 'to', 'maintain', 'wifi', 'connectivity', 'during', 'sleep', 'mode']\n",
      "After stop words removal: ['maintain', 'wifi', 'connectivity', 'sleep', 'mode']\n",
      "After stemming with porters algorithm: ['maintain', 'wifi', 'connect', 'sleep', 'mode']\n",
      "Tokenized sentence: ['wifi', 'disconnecting', 'after', 'hibernation']\n",
      "After stop words removal: ['wifi', 'disconnecting', 'hibernation']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['wifi', 'disconnec', 'hibern']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'stop', 'my', 'device', 'from', 'disconnecting', 'from', 'wifi']\n",
      "After stop words removal: ['stop', 'device', 'disconnecting', 'wifi']\n",
      "disconnect\n",
      "After stemming with porters algorithm: ['stop', 'devic', 'disconnec', 'wifi']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'connect', 'to', 'a', 'hidden', 'wifi', 'network']\n",
      "After stop words removal: ['connect', 'hidden', 'wifi', 'network']\n",
      "After stemming with porters algorithm: ['connect', 'hidden', 'wifi', 'network']\n",
      "Tokenized sentence: ['connecting', 'to', 'a', 'hidden', 'ssid']\n",
      "After stop words removal: ['connecting', 'hidden', 'ssid']\n",
      "connect\n",
      "After stemming with porters algorithm: ['connec', 'hidden', 'ssid']\n",
      "Tokenized sentence: ['hidden', 'wifi', 'network', 'connection', 'issues']\n",
      "After stop words removal: ['hidden', 'wifi', 'network', 'connection', 'issues']\n",
      "After stemming with porters algorithm: ['hidden', 'wifi', 'network', 'connect', 'issu']\n",
      "Tokenized sentence: ['can', 't', 'connect', 'to', 'hidden', 'wifi']\n",
      "After stop words removal: ['connect', 'hidden', 'wifi']\n",
      "After stemming with porters algorithm: ['connect', 'hidden', 'wifi']\n",
      "Tokenized sentence: ['how', 'to', 'find', 'and', 'connect', 'to', 'hidden', 'wifi']\n",
      "After stop words removal: ['find', 'connect', 'hidden', 'wifi']\n",
      "After stemming with porters algorithm: ['find', 'connect', 'hidden', 'wifi']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'make', 'my', 'wifi', 'network', 'hidden']\n",
      "After stop words removal: ['make', 'wifi', 'network', 'hidden']\n",
      "After stemming with porters algorithm: ['make', 'wifi', 'network', 'hidden']\n",
      "Tokenized sentence: ['why', 'can', 't', 'i', 'see', 'my', 'hidden', 'wifi', 'network']\n",
      "After stop words removal: ['see', 'hidden', 'wifi', 'network']\n",
      "After stemming with porters algorithm: ['see', 'hidden', 'wifi', 'network']\n",
      "Tokenized sentence: ['what', 'should', 'i', 'do', 'if', 'my', 'hidden', 'wifi', 'network', 'isn', 't', 'showing']\n",
      "After stop words removal: ['hidden', 'wifi', 'network', 'showing']\n",
      "show\n",
      "After stemming with porters algorithm: ['hidden', 'wifi', 'network', 'showe']\n",
      "Tokenized sentence: ['can', 'i', 'connect', 'to', 'a', 'hidden', 'wifi', 'network', 'manually']\n",
      "After stop words removal: ['connect', 'hidden', 'wifi', 'network', 'manually']\n",
      "After stemming with porters algorithm: ['connect', 'hidden', 'wifi', 'network', 'manual']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'find', 'the', 'password', 'for', 'a', 'hidden', 'wifi', 'network']\n",
      "After stop words removal: ['find', 'password', 'hidden', 'wifi', 'network']\n",
      "After stemming with porters algorithm: ['find', 'password', 'hidden', 'wifi', 'network']\n",
      "Tokenized sentence: ['gpu', 'fan', 'not', 'spinning', 'properly']\n",
      "After stop words removal: ['gpu', 'fan', 'spinning', 'properly']\n",
      "spinn\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'spin', 'properli']\n",
      "Tokenized sentence: ['fan', 'issues']\n",
      "After stop words removal: ['fan', 'issues']\n",
      "After stemming with porters algorithm: ['fan', 'issu']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'gpu', 'fan', 'not', 'working']\n",
      "After stop words removal: ['gpu', 'fan', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'wor']\n",
      "Tokenized sentence: ['gpu', 'fan', 'not', 'spinning', 'at', 'all']\n",
      "After stop words removal: ['gpu', 'fan', 'spinning']\n",
      "spinn\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'spin']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'a', 'stuck', 'gpu', 'fan']\n",
      "After stop words removal: ['fix', 'stuck', 'gpu', 'fan']\n",
      "After stemming with porters algorithm: ['fix', 'stuck', 'gpu', 'fan']\n",
      "Tokenized sentence: ['gpu', 'fan', 'noise', 'issues']\n",
      "After stop words removal: ['gpu', 'fan', 'noise', 'issues']\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'nois', 'issu']\n",
      "Tokenized sentence: ['my', 'gpu', 'fan', 'is', 'making', 'a', 'weird', 'noise']\n",
      "After stop words removal: ['gpu', 'fan', 'making', 'weird', 'noise']\n",
      "mak\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'make', 'weird', 'nois']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'test', 'a', 'gpu', 'fan']\n",
      "After stop words removal: ['test', 'gpu', 'fan']\n",
      "After stemming with porters algorithm: ['test', 'gpu', 'fan']\n",
      "Tokenized sentence: ['gpu', 'overheating', 'due', 'to', 'fan', 'issues']\n",
      "After stop words removal: ['gpu', 'overheating', 'due', 'fan', 'issues']\n",
      "overheat\n",
      "overheate\n",
      "After stemming with porters algorithm: ['gpu', 'overh', 'due', 'fan', 'issu']\n",
      "Tokenized sentence: ['why', 'is', 'my', 'gpu', 'fan', 'spinning', 'slowly']\n",
      "After stop words removal: ['gpu', 'fan', 'spinning', 'slowly']\n",
      "spinn\n",
      "After stemming with porters algorithm: ['gpu', 'fan', 'spin', 'slowli']\n",
      "Tokenized sentence: ['gpu', 'causing', 'black', 'screen', 'during', 'boot']\n",
      "After stop words removal: ['gpu', 'causing', 'black', 'screen', 'boot']\n",
      "caus\n",
      "After stemming with porters algorithm: ['gpu', 'caus', 'black', 'screen', 'boot']\n",
      "Tokenized sentence: ['black', 'screen', 'on', 'startup']\n",
      "After stop words removal: ['black', 'screen', 'startup']\n",
      "After stemming with porters algorithm: ['black', 'screen', 'startup']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'screen', 'go', 'black', 'after', 'installing', 'a', 'new', 'gpu']\n",
      "After stop words removal: ['screen', 'go', 'black', 'installing', 'new', 'gpu']\n",
      "install\n",
      "After stemming with porters algorithm: ['screen', 'black', 'instal', 'new', 'gpu']\n",
      "Tokenized sentence: ['how', 'do', 'i', 'fix', 'a', 'black', 'screen', 'caused', 'by', 'gpu']\n",
      "After stop words removal: ['fix', 'black', 'screen', 'caused', 'gpu']\n",
      "After stemming with porters algorithm: ['fix', 'black', 'screen', 'caus', 'gpu']\n",
      "Tokenized sentence: ['gpu', 'causing', 'boot', 'failure']\n",
      "After stop words removal: ['gpu', 'causing', 'boot', 'failure']\n",
      "caus\n",
      "After stemming with porters algorithm: ['gpu', 'caus', 'boot', 'failur']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'computer', 'show', 'a', 'black', 'screen', 'after', 'the', 'gpu', 'update']\n",
      "After stop words removal: ['computer', 'show', 'black', 'screen', 'gpu', 'update']\n",
      "After stemming with porters algorithm: ['comput', 'show', 'black', 'screen', 'gpu', 'updat']\n",
      "Tokenized sentence: ['how', 'can', 'i', 'solve', 'black', 'screen', 'issues', 'with', 'gpu']\n",
      "After stop words removal: ['solve', 'black', 'screen', 'issues', 'gpu']\n",
      "After stemming with porters algorithm: ['solv', 'black', 'screen', 'issu', 'gpu']\n",
      "Tokenized sentence: ['what', 'should', 'i', 'do', 'if', 'my', 'screen', 'turns', 'black', 'during', 'gaming']\n",
      "After stop words removal: ['screen', 'turns', 'black', 'gaming']\n",
      "gam\n",
      "After stemming with porters algorithm: ['screen', 'turn', 'black', 'game']\n",
      "Tokenized sentence: ['gpu', 'black', 'screen', 'during', 'login']\n",
      "After stop words removal: ['gpu', 'black', 'screen', 'login']\n",
      "After stemming with porters algorithm: ['gpu', 'black', 'screen', 'login']\n",
      "Tokenized sentence: ['why', 'does', 'my', 'gpu', 'cause', 'a', 'black', 'screen']\n",
      "After stop words removal: ['gpu', 'cause', 'black', 'screen']\n",
      "After stemming with porters algorithm: ['gpu', 'caus', 'black', 'screen']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('updated_corpus/updated_corpus.json', 'r') as f:\n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "\n",
    "        \n",
    "\n",
    "def preprocess_data(data):\n",
    "    rows = []\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            processed_pattern = pipeline(pattern)\n",
    "            row = {\n",
    "                \"input_text\": \" \".join(processed_pattern),\n",
    "                \"target\": intent[\"tag\"]\n",
    "            }\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Transform the data to CSV format\n",
    "preprocessed_data = preprocess_data(data)\n",
    "\n",
    "with open(\"updated_preprocessed_data.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"input_text\", \"target\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for row in preprocessed_data:\n",
    "        writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 20)\n",
      "(288,)\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anudeep/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.0497 - loss: 3.5846 - val_accuracy: 0.0000e+00 - val_loss: 3.5850\n",
      "Epoch 2/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0471 - loss: 3.5839 - val_accuracy: 0.0000e+00 - val_loss: 3.5899\n",
      "Epoch 3/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0408 - loss: 3.5801 - val_accuracy: 0.0000e+00 - val_loss: 3.5963\n",
      "Epoch 4/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0261 - loss: 3.5808 - val_accuracy: 0.0000e+00 - val_loss: 3.6000\n",
      "Epoch 5/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0308 - loss: 3.5791 - val_accuracy: 0.0000e+00 - val_loss: 3.6051\n",
      "Epoch 6/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0458 - loss: 3.5776 - val_accuracy: 0.0000e+00 - val_loss: 3.6165\n",
      "Epoch 7/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0145 - loss: 3.5822 - val_accuracy: 0.0000e+00 - val_loss: 3.6176\n",
      "Epoch 8/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0485 - loss: 3.5728 - val_accuracy: 0.0000e+00 - val_loss: 3.6231\n",
      "Epoch 9/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0513 - loss: 3.5775 - val_accuracy: 0.0000e+00 - val_loss: 3.6104\n",
      "Epoch 10/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0759 - loss: 3.5660 - val_accuracy: 0.0000e+00 - val_loss: 3.6169\n",
      "Epoch 11/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0487 - loss: 3.5597 - val_accuracy: 0.0000e+00 - val_loss: 3.6284\n",
      "Epoch 12/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0597 - loss: 3.5587 - val_accuracy: 0.0000e+00 - val_loss: 3.6616\n",
      "Epoch 13/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0437 - loss: 3.5449 - val_accuracy: 0.0000e+00 - val_loss: 3.6093\n",
      "Epoch 14/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0515 - loss: 3.5083 - val_accuracy: 0.0139 - val_loss: 3.7080\n",
      "Epoch 15/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0699 - loss: 3.4821 - val_accuracy: 0.0278 - val_loss: 3.5356\n",
      "Epoch 16/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1069 - loss: 3.3635 - val_accuracy: 0.0278 - val_loss: 3.4471\n",
      "Epoch 17/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0968 - loss: 3.1905 - val_accuracy: 0.0694 - val_loss: 3.3045\n",
      "Epoch 18/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0961 - loss: 3.0337 - val_accuracy: 0.0694 - val_loss: 3.2321\n",
      "Epoch 19/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1558 - loss: 2.8736 - val_accuracy: 0.1111 - val_loss: 3.0407\n",
      "Epoch 20/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1240 - loss: 2.7682 - val_accuracy: 0.1250 - val_loss: 2.9108\n",
      "Epoch 21/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2467 - loss: 2.6067 - val_accuracy: 0.1667 - val_loss: 2.8530\n",
      "Epoch 22/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2410 - loss: 2.4751 - val_accuracy: 0.2083 - val_loss: 2.6409\n",
      "Epoch 23/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1924 - loss: 2.4239 - val_accuracy: 0.1944 - val_loss: 2.5150\n",
      "Epoch 24/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3216 - loss: 2.3570 - val_accuracy: 0.1667 - val_loss: 2.5469\n",
      "Epoch 25/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3129 - loss: 2.2207 - val_accuracy: 0.2500 - val_loss: 2.4882\n",
      "Epoch 26/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3480 - loss: 2.1452 - val_accuracy: 0.2361 - val_loss: 2.4326\n",
      "Epoch 27/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3065 - loss: 2.0643 - val_accuracy: 0.3472 - val_loss: 2.1896\n",
      "Epoch 28/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4251 - loss: 1.9224 - val_accuracy: 0.3194 - val_loss: 2.1839\n",
      "Epoch 29/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4534 - loss: 1.8742 - val_accuracy: 0.3750 - val_loss: 2.0956\n",
      "Epoch 30/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4118 - loss: 1.8299 - val_accuracy: 0.2778 - val_loss: 2.0506\n",
      "Epoch 31/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4405 - loss: 1.7184 - val_accuracy: 0.3056 - val_loss: 2.1017\n",
      "Epoch 32/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4176 - loss: 1.7214 - val_accuracy: 0.4028 - val_loss: 1.9546\n",
      "Epoch 33/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4701 - loss: 1.6148 - val_accuracy: 0.4028 - val_loss: 1.8305\n",
      "Epoch 34/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5258 - loss: 1.5327 - val_accuracy: 0.4167 - val_loss: 1.7936\n",
      "Epoch 35/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4899 - loss: 1.5336 - val_accuracy: 0.4306 - val_loss: 1.8489\n",
      "Epoch 36/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5319 - loss: 1.4472 - val_accuracy: 0.3611 - val_loss: 1.9074\n",
      "Epoch 37/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5817 - loss: 1.3626 - val_accuracy: 0.4861 - val_loss: 1.6912\n",
      "Epoch 38/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6180 - loss: 1.3315 - val_accuracy: 0.4722 - val_loss: 1.7082\n",
      "Epoch 39/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6250 - loss: 1.2818 - val_accuracy: 0.4306 - val_loss: 1.7106\n",
      "Epoch 40/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6425 - loss: 1.2535 - val_accuracy: 0.4583 - val_loss: 1.6240\n",
      "Epoch 41/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7012 - loss: 1.1393 - val_accuracy: 0.5000 - val_loss: 1.6710\n",
      "Epoch 42/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6623 - loss: 1.1218 - val_accuracy: 0.4861 - val_loss: 1.6266\n",
      "Epoch 43/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6356 - loss: 1.1450 - val_accuracy: 0.4306 - val_loss: 1.6371\n",
      "Epoch 44/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6591 - loss: 1.1393 - val_accuracy: 0.4722 - val_loss: 1.6791\n",
      "Epoch 45/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7100 - loss: 1.0761 - val_accuracy: 0.4861 - val_loss: 1.5961\n",
      "Epoch 46/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6961 - loss: 1.0509 - val_accuracy: 0.5000 - val_loss: 1.5259\n",
      "Epoch 47/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7280 - loss: 0.9725 - val_accuracy: 0.4167 - val_loss: 1.6573\n",
      "Epoch 48/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7033 - loss: 0.9249 - val_accuracy: 0.4722 - val_loss: 1.5223\n",
      "Epoch 49/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7773 - loss: 0.9097 - val_accuracy: 0.5417 - val_loss: 1.5032\n",
      "Epoch 50/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7330 - loss: 0.8712 - val_accuracy: 0.5278 - val_loss: 1.5050\n",
      "Epoch 51/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7812 - loss: 0.8405 - val_accuracy: 0.5139 - val_loss: 1.6298\n",
      "Epoch 52/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7106 - loss: 0.8630 - val_accuracy: 0.5278 - val_loss: 1.5424\n",
      "Epoch 53/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7514 - loss: 0.8110 - val_accuracy: 0.5139 - val_loss: 1.6087\n",
      "Epoch 54/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7577 - loss: 0.8268 - val_accuracy: 0.4861 - val_loss: 1.5874\n",
      "Epoch 55/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7904 - loss: 0.7056 - val_accuracy: 0.5417 - val_loss: 1.5056\n",
      "Epoch 56/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7534 - loss: 0.7709 - val_accuracy: 0.5972 - val_loss: 1.5612\n",
      "Epoch 57/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8217 - loss: 0.7172 - val_accuracy: 0.5139 - val_loss: 1.5803\n",
      "Epoch 58/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7813 - loss: 0.7088 - val_accuracy: 0.5417 - val_loss: 1.5504\n",
      "Epoch 59/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7780 - loss: 0.7566 - val_accuracy: 0.5139 - val_loss: 1.5382\n",
      "Epoch 60/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8462 - loss: 0.5702 - val_accuracy: 0.5556 - val_loss: 1.5131\n",
      "Epoch 61/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8162 - loss: 0.6454 - val_accuracy: 0.5833 - val_loss: 1.5328\n",
      "Epoch 62/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8412 - loss: 0.6721 - val_accuracy: 0.6111 - val_loss: 1.5668\n",
      "Epoch 63/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8675 - loss: 0.5781 - val_accuracy: 0.5972 - val_loss: 1.4911\n",
      "Epoch 64/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8356 - loss: 0.5749 - val_accuracy: 0.5278 - val_loss: 1.6436\n",
      "Epoch 65/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7999 - loss: 0.6189 - val_accuracy: 0.5556 - val_loss: 1.5589\n",
      "Epoch 66/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8645 - loss: 0.5478 - val_accuracy: 0.5972 - val_loss: 1.5325\n",
      "Epoch 67/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8930 - loss: 0.4687 - val_accuracy: 0.6250 - val_loss: 1.5039\n",
      "Epoch 68/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8240 - loss: 0.6107 - val_accuracy: 0.6111 - val_loss: 1.5358\n",
      "Epoch 69/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8548 - loss: 0.4974 - val_accuracy: 0.6111 - val_loss: 1.5187\n",
      "Epoch 70/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8523 - loss: 0.5550 - val_accuracy: 0.5972 - val_loss: 1.5452\n",
      "Epoch 71/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8281 - loss: 0.5273 - val_accuracy: 0.6389 - val_loss: 1.6420\n",
      "Epoch 72/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8484 - loss: 0.4867 - val_accuracy: 0.6389 - val_loss: 1.4328\n",
      "Epoch 73/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8770 - loss: 0.4623 - val_accuracy: 0.5556 - val_loss: 1.7410\n",
      "Epoch 74/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8331 - loss: 0.5652 - val_accuracy: 0.6250 - val_loss: 1.4965\n",
      "Epoch 75/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8661 - loss: 0.5041 - val_accuracy: 0.6528 - val_loss: 1.5656\n",
      "Epoch 76/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8728 - loss: 0.4757 - val_accuracy: 0.5833 - val_loss: 1.5774\n",
      "Epoch 77/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9053 - loss: 0.3988 - val_accuracy: 0.6111 - val_loss: 1.5425\n",
      "Epoch 78/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8938 - loss: 0.4298 - val_accuracy: 0.5833 - val_loss: 1.5633\n",
      "Epoch 79/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8612 - loss: 0.3832 - val_accuracy: 0.6389 - val_loss: 1.5733\n",
      "Epoch 80/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8920 - loss: 0.4361 - val_accuracy: 0.6389 - val_loss: 1.5720\n",
      "Epoch 81/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8711 - loss: 0.4054 - val_accuracy: 0.5972 - val_loss: 1.5300\n",
      "Epoch 82/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9271 - loss: 0.3482 - val_accuracy: 0.6528 - val_loss: 1.5712\n",
      "Epoch 83/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8531 - loss: 0.4334 - val_accuracy: 0.6528 - val_loss: 1.5470\n",
      "Epoch 84/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8727 - loss: 0.4270 - val_accuracy: 0.6389 - val_loss: 1.6517\n",
      "Epoch 85/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8833 - loss: 0.3792 - val_accuracy: 0.6667 - val_loss: 1.5671\n",
      "Epoch 86/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8849 - loss: 0.3813 - val_accuracy: 0.6389 - val_loss: 1.5382\n",
      "Epoch 87/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9162 - loss: 0.3482 - val_accuracy: 0.6528 - val_loss: 1.5608\n",
      "Epoch 88/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8661 - loss: 0.3979 - val_accuracy: 0.6944 - val_loss: 1.4705\n",
      "Epoch 89/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8730 - loss: 0.4113 - val_accuracy: 0.6389 - val_loss: 1.5975\n",
      "Epoch 90/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8841 - loss: 0.4074 - val_accuracy: 0.6528 - val_loss: 1.4977\n",
      "Epoch 91/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8769 - loss: 0.3677 - val_accuracy: 0.6806 - val_loss: 1.5329\n",
      "Epoch 92/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8978 - loss: 0.3751 - val_accuracy: 0.6528 - val_loss: 1.5537\n",
      "Epoch 93/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8720 - loss: 0.3459 - val_accuracy: 0.6667 - val_loss: 1.6825\n",
      "Epoch 94/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8855 - loss: 0.3550 - val_accuracy: 0.6389 - val_loss: 1.5855\n",
      "Epoch 95/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8890 - loss: 0.3457 - val_accuracy: 0.6806 - val_loss: 1.5076\n",
      "Epoch 96/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9403 - loss: 0.3147 - val_accuracy: 0.6944 - val_loss: 1.4935\n",
      "Epoch 97/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9372 - loss: 0.3026 - val_accuracy: 0.6250 - val_loss: 1.5570\n",
      "Epoch 98/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9000 - loss: 0.3112 - val_accuracy: 0.6250 - val_loss: 1.5371\n",
      "Epoch 99/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9223 - loss: 0.3332 - val_accuracy: 0.6250 - val_loss: 1.6503\n",
      "Epoch 100/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9182 - loss: 0.3231 - val_accuracy: 0.6389 - val_loss: 1.5397\n",
      "Epoch 101/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9246 - loss: 0.3188 - val_accuracy: 0.6389 - val_loss: 1.5406\n",
      "Epoch 102/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9253 - loss: 0.3099 - val_accuracy: 0.6389 - val_loss: 1.5331\n",
      "Epoch 103/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9241 - loss: 0.3155 - val_accuracy: 0.5972 - val_loss: 1.6036\n",
      "Epoch 104/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9143 - loss: 0.3297 - val_accuracy: 0.6528 - val_loss: 1.5091\n",
      "Epoch 105/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9354 - loss: 0.2785 - val_accuracy: 0.6389 - val_loss: 1.7242\n",
      "Epoch 106/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9163 - loss: 0.2922 - val_accuracy: 0.6667 - val_loss: 1.6443\n",
      "Epoch 107/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9373 - loss: 0.2807 - val_accuracy: 0.6250 - val_loss: 1.7113\n",
      "Epoch 108/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9241 - loss: 0.2996 - val_accuracy: 0.6250 - val_loss: 1.6449\n",
      "Epoch 109/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9461 - loss: 0.2465 - val_accuracy: 0.6111 - val_loss: 1.9204\n",
      "Epoch 110/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9432 - loss: 0.2528 - val_accuracy: 0.6389 - val_loss: 1.5245\n",
      "Epoch 111/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9243 - loss: 0.2685 - val_accuracy: 0.6528 - val_loss: 1.6915\n",
      "Epoch 112/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9434 - loss: 0.2605 - val_accuracy: 0.6667 - val_loss: 1.6119\n",
      "Epoch 113/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9329 - loss: 0.2456 - val_accuracy: 0.6250 - val_loss: 1.7197\n",
      "Epoch 114/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9369 - loss: 0.2261 - val_accuracy: 0.6389 - val_loss: 1.7017\n",
      "Epoch 115/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9100 - loss: 0.2712 - val_accuracy: 0.6389 - val_loss: 1.6377\n",
      "Epoch 116/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9453 - loss: 0.2275 - val_accuracy: 0.6389 - val_loss: 1.6393\n",
      "Epoch 117/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9091 - loss: 0.2767 - val_accuracy: 0.6389 - val_loss: 1.6583\n",
      "Epoch 118/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9441 - loss: 0.2444 - val_accuracy: 0.6250 - val_loss: 1.6091\n",
      "Epoch 119/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9520 - loss: 0.2468 - val_accuracy: 0.6389 - val_loss: 1.6344\n",
      "Epoch 120/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9238 - loss: 0.2383 - val_accuracy: 0.6528 - val_loss: 1.6212\n",
      "Epoch 121/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9298 - loss: 0.2114 - val_accuracy: 0.6250 - val_loss: 1.5729\n",
      "Epoch 122/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9267 - loss: 0.2590 - val_accuracy: 0.6250 - val_loss: 1.7944\n",
      "Epoch 123/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9522 - loss: 0.2250 - val_accuracy: 0.6806 - val_loss: 1.6072\n",
      "Epoch 124/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9576 - loss: 0.2212 - val_accuracy: 0.6111 - val_loss: 1.6471\n",
      "Epoch 125/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9324 - loss: 0.2256 - val_accuracy: 0.6389 - val_loss: 1.8418\n",
      "Epoch 126/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9411 - loss: 0.2148 - val_accuracy: 0.6667 - val_loss: 1.6909\n",
      "Epoch 127/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9297 - loss: 0.2394 - val_accuracy: 0.6389 - val_loss: 1.6975\n",
      "Epoch 128/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9374 - loss: 0.2033 - val_accuracy: 0.6806 - val_loss: 1.6240\n",
      "Epoch 129/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9518 - loss: 0.2219 - val_accuracy: 0.6667 - val_loss: 1.7264\n",
      "Epoch 130/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9663 - loss: 0.1558 - val_accuracy: 0.6389 - val_loss: 1.7034\n",
      "Epoch 131/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9244 - loss: 0.2459 - val_accuracy: 0.6528 - val_loss: 1.8891\n",
      "Epoch 132/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9570 - loss: 0.2126 - val_accuracy: 0.6250 - val_loss: 1.5558\n",
      "Epoch 133/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9426 - loss: 0.2050 - val_accuracy: 0.6250 - val_loss: 1.7059\n",
      "Epoch 134/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9515 - loss: 0.2229 - val_accuracy: 0.6528 - val_loss: 1.6934\n",
      "Epoch 135/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9468 - loss: 0.1997 - val_accuracy: 0.6389 - val_loss: 1.6275\n",
      "Epoch 136/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9379 - loss: 0.2139 - val_accuracy: 0.6528 - val_loss: 1.7169\n",
      "Epoch 137/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9484 - loss: 0.1995 - val_accuracy: 0.6250 - val_loss: 1.7761\n",
      "Epoch 138/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9620 - loss: 0.1651 - val_accuracy: 0.6250 - val_loss: 1.8692\n",
      "Epoch 139/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9610 - loss: 0.1633 - val_accuracy: 0.6667 - val_loss: 1.5745\n",
      "Epoch 140/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9338 - loss: 0.2126 - val_accuracy: 0.6667 - val_loss: 1.6138\n",
      "Epoch 141/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9495 - loss: 0.1885 - val_accuracy: 0.6389 - val_loss: 1.5300\n",
      "Epoch 142/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9540 - loss: 0.1876 - val_accuracy: 0.6250 - val_loss: 1.6625\n",
      "Epoch 143/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9751 - loss: 0.1551 - val_accuracy: 0.6250 - val_loss: 1.7075\n",
      "Epoch 144/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9661 - loss: 0.1737 - val_accuracy: 0.6389 - val_loss: 1.6399\n",
      "Epoch 145/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9624 - loss: 0.1752 - val_accuracy: 0.5972 - val_loss: 1.8631\n",
      "Epoch 146/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9686 - loss: 0.1345 - val_accuracy: 0.6111 - val_loss: 1.9204\n",
      "Epoch 147/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9412 - loss: 0.2081 - val_accuracy: 0.6250 - val_loss: 1.6857\n",
      "Epoch 148/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9667 - loss: 0.2009 - val_accuracy: 0.6389 - val_loss: 1.6399\n",
      "Epoch 149/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9554 - loss: 0.1729 - val_accuracy: 0.6250 - val_loss: 1.7651\n",
      "Epoch 150/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9692 - loss: 0.1387 - val_accuracy: 0.6389 - val_loss: 1.8321\n",
      "Epoch 151/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9587 - loss: 0.1787 - val_accuracy: 0.6389 - val_loss: 1.7210\n",
      "Epoch 152/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9642 - loss: 0.1530 - val_accuracy: 0.6389 - val_loss: 1.6098\n",
      "Epoch 153/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9537 - loss: 0.2048 - val_accuracy: 0.6667 - val_loss: 1.7759\n",
      "Epoch 154/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9621 - loss: 0.1675 - val_accuracy: 0.6389 - val_loss: 1.7783\n",
      "Epoch 155/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9567 - loss: 0.1386 - val_accuracy: 0.6389 - val_loss: 1.7736\n",
      "Epoch 156/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9627 - loss: 0.1465 - val_accuracy: 0.6528 - val_loss: 1.6353\n",
      "Epoch 157/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9533 - loss: 0.1586 - val_accuracy: 0.6389 - val_loss: 1.7237\n",
      "Epoch 158/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9508 - loss: 0.1582 - val_accuracy: 0.6528 - val_loss: 1.7376\n",
      "Epoch 159/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9471 - loss: 0.1777 - val_accuracy: 0.6667 - val_loss: 1.9080\n",
      "Epoch 160/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9754 - loss: 0.1184 - val_accuracy: 0.6389 - val_loss: 1.8115\n",
      "Epoch 161/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9375 - loss: 0.1766 - val_accuracy: 0.6250 - val_loss: 1.9137\n",
      "Epoch 162/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9372 - loss: 0.1790 - val_accuracy: 0.6250 - val_loss: 1.8273\n",
      "Epoch 163/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9612 - loss: 0.1558 - val_accuracy: 0.6250 - val_loss: 1.8440\n",
      "Epoch 164/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9681 - loss: 0.1322 - val_accuracy: 0.6528 - val_loss: 1.8006\n",
      "Epoch 165/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9480 - loss: 0.1683 - val_accuracy: 0.6528 - val_loss: 1.8913\n",
      "Epoch 166/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9490 - loss: 0.1620 - val_accuracy: 0.6389 - val_loss: 1.7571\n",
      "Epoch 167/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9532 - loss: 0.1761 - val_accuracy: 0.6528 - val_loss: 1.6568\n",
      "Epoch 168/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9753 - loss: 0.1530 - val_accuracy: 0.6528 - val_loss: 1.9001\n",
      "Epoch 169/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9721 - loss: 0.1244 - val_accuracy: 0.6528 - val_loss: 1.8971\n",
      "Epoch 170/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9601 - loss: 0.1320 - val_accuracy: 0.6667 - val_loss: 1.9297\n",
      "Epoch 171/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9782 - loss: 0.1141 - val_accuracy: 0.6389 - val_loss: 1.9794\n",
      "Epoch 172/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9605 - loss: 0.1627 - val_accuracy: 0.6111 - val_loss: 2.2016\n",
      "Epoch 173/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9371 - loss: 0.1726 - val_accuracy: 0.6389 - val_loss: 1.8291\n",
      "Epoch 174/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9666 - loss: 0.1531 - val_accuracy: 0.6389 - val_loss: 1.8242\n",
      "Epoch 175/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9744 - loss: 0.1196 - val_accuracy: 0.5972 - val_loss: 2.0118\n",
      "Epoch 176/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9775 - loss: 0.0939 - val_accuracy: 0.6250 - val_loss: 2.1319\n",
      "Epoch 177/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9761 - loss: 0.1514 - val_accuracy: 0.6389 - val_loss: 1.8630\n",
      "Epoch 178/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9481 - loss: 0.1512 - val_accuracy: 0.6528 - val_loss: 1.7978\n",
      "Epoch 179/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9654 - loss: 0.1058 - val_accuracy: 0.6667 - val_loss: 1.6744\n",
      "Epoch 180/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9734 - loss: 0.1201 - val_accuracy: 0.6389 - val_loss: 1.7174\n",
      "Epoch 181/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9814 - loss: 0.1159 - val_accuracy: 0.6528 - val_loss: 1.8500\n",
      "Epoch 182/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9676 - loss: 0.1099 - val_accuracy: 0.6250 - val_loss: 1.8608\n",
      "Epoch 183/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9722 - loss: 0.0968 - val_accuracy: 0.6389 - val_loss: 1.8184\n",
      "Epoch 184/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9908 - loss: 0.0627 - val_accuracy: 0.6389 - val_loss: 1.8865\n",
      "Epoch 185/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9716 - loss: 0.1113 - val_accuracy: 0.6111 - val_loss: 1.9589\n",
      "Epoch 186/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9725 - loss: 0.1254 - val_accuracy: 0.6389 - val_loss: 2.0193\n",
      "Epoch 187/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9775 - loss: 0.0795 - val_accuracy: 0.6250 - val_loss: 2.0599\n",
      "Epoch 188/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9481 - loss: 0.1476 - val_accuracy: 0.6111 - val_loss: 1.9988\n",
      "Epoch 189/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9740 - loss: 0.1627 - val_accuracy: 0.6528 - val_loss: 1.9767\n",
      "Epoch 190/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9681 - loss: 0.1049 - val_accuracy: 0.6250 - val_loss: 2.1139\n",
      "Epoch 191/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9764 - loss: 0.1024 - val_accuracy: 0.6250 - val_loss: 1.8937\n",
      "Epoch 192/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9742 - loss: 0.1368 - val_accuracy: 0.6250 - val_loss: 1.8309\n",
      "Epoch 193/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9604 - loss: 0.1194 - val_accuracy: 0.6250 - val_loss: 1.8553\n",
      "Epoch 194/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9873 - loss: 0.0859 - val_accuracy: 0.6250 - val_loss: 1.9381\n",
      "Epoch 195/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9640 - loss: 0.1146 - val_accuracy: 0.6250 - val_loss: 1.8990\n",
      "Epoch 196/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9734 - loss: 0.1029 - val_accuracy: 0.6250 - val_loss: 1.8789\n",
      "Epoch 197/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9853 - loss: 0.1023 - val_accuracy: 0.6389 - val_loss: 1.9200\n",
      "Epoch 198/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9586 - loss: 0.1251 - val_accuracy: 0.6528 - val_loss: 2.0859\n",
      "Epoch 199/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9912 - loss: 0.0853 - val_accuracy: 0.6528 - val_loss: 1.9018\n",
      "Epoch 200/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9759 - loss: 0.0957 - val_accuracy: 0.6528 - val_loss: 1.9066\n",
      "Epoch 201/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9843 - loss: 0.1279 - val_accuracy: 0.6389 - val_loss: 2.0030\n",
      "Epoch 202/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9631 - loss: 0.1391 - val_accuracy: 0.6389 - val_loss: 2.0547\n",
      "Epoch 203/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9616 - loss: 0.1507 - val_accuracy: 0.6389 - val_loss: 1.9462\n",
      "Epoch 204/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9825 - loss: 0.0985 - val_accuracy: 0.6389 - val_loss: 1.8898\n",
      "Epoch 205/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9834 - loss: 0.0880 - val_accuracy: 0.6250 - val_loss: 2.0400\n",
      "Epoch 206/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9563 - loss: 0.1018 - val_accuracy: 0.6389 - val_loss: 1.9971\n",
      "Epoch 207/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9725 - loss: 0.1125 - val_accuracy: 0.6111 - val_loss: 2.1137\n",
      "Epoch 208/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9908 - loss: 0.0796 - val_accuracy: 0.6250 - val_loss: 1.9734\n",
      "Epoch 209/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9645 - loss: 0.1684 - val_accuracy: 0.6389 - val_loss: 1.9006\n",
      "Epoch 210/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9586 - loss: 0.1099 - val_accuracy: 0.6250 - val_loss: 1.8714\n",
      "Epoch 211/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9745 - loss: 0.1232 - val_accuracy: 0.6250 - val_loss: 2.1959\n",
      "Epoch 212/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9713 - loss: 0.1208 - val_accuracy: 0.6111 - val_loss: 1.9667\n",
      "Epoch 213/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9755 - loss: 0.0753 - val_accuracy: 0.6250 - val_loss: 2.0524\n",
      "Epoch 214/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9899 - loss: 0.0750 - val_accuracy: 0.6111 - val_loss: 2.1703\n",
      "Epoch 215/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9740 - loss: 0.1065 - val_accuracy: 0.6111 - val_loss: 2.0186\n",
      "Epoch 216/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9800 - loss: 0.0785 - val_accuracy: 0.6111 - val_loss: 2.0285\n",
      "Epoch 217/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9917 - loss: 0.0661 - val_accuracy: 0.5972 - val_loss: 2.0645\n",
      "Epoch 218/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9456 - loss: 0.1221 - val_accuracy: 0.6111 - val_loss: 2.0751\n",
      "Epoch 219/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9843 - loss: 0.0950 - val_accuracy: 0.6389 - val_loss: 2.0511\n",
      "Epoch 220/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9775 - loss: 0.0917 - val_accuracy: 0.6389 - val_loss: 1.9796\n",
      "Epoch 221/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9810 - loss: 0.0739 - val_accuracy: 0.6389 - val_loss: 2.0847\n",
      "Epoch 222/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9936 - loss: 0.0637 - val_accuracy: 0.5972 - val_loss: 2.1987\n",
      "Epoch 223/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9707 - loss: 0.1068 - val_accuracy: 0.5833 - val_loss: 1.9570\n",
      "Epoch 224/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9648 - loss: 0.0971 - val_accuracy: 0.5972 - val_loss: 1.9541\n",
      "Epoch 225/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9757 - loss: 0.0927 - val_accuracy: 0.6250 - val_loss: 2.1542\n",
      "Epoch 226/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9820 - loss: 0.1065 - val_accuracy: 0.6389 - val_loss: 1.9841\n",
      "Epoch 227/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9877 - loss: 0.0760 - val_accuracy: 0.6250 - val_loss: 2.0346\n",
      "Epoch 228/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9908 - loss: 0.0961 - val_accuracy: 0.6250 - val_loss: 2.1294\n",
      "Epoch 229/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9864 - loss: 0.0678 - val_accuracy: 0.6111 - val_loss: 2.2561\n",
      "Epoch 230/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9836 - loss: 0.0885 - val_accuracy: 0.5972 - val_loss: 2.1168\n",
      "Epoch 231/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9759 - loss: 0.1275 - val_accuracy: 0.6250 - val_loss: 2.1624\n",
      "Epoch 232/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9796 - loss: 0.0957 - val_accuracy: 0.6389 - val_loss: 2.1260\n",
      "Epoch 233/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.1217 - val_accuracy: 0.6528 - val_loss: 1.9286\n",
      "Epoch 234/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9721 - loss: 0.0940 - val_accuracy: 0.6250 - val_loss: 2.0084\n",
      "Epoch 235/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9684 - loss: 0.1037 - val_accuracy: 0.6389 - val_loss: 2.0677\n",
      "Epoch 236/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9894 - loss: 0.0650 - val_accuracy: 0.6250 - val_loss: 2.1490\n",
      "Epoch 237/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9759 - loss: 0.0820 - val_accuracy: 0.6250 - val_loss: 2.1515\n",
      "Epoch 238/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9858 - loss: 0.0633 - val_accuracy: 0.6389 - val_loss: 2.1668\n",
      "Epoch 239/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9655 - loss: 0.1034 - val_accuracy: 0.6528 - val_loss: 2.1485\n",
      "Epoch 240/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9819 - loss: 0.0944 - val_accuracy: 0.5972 - val_loss: 2.1487\n",
      "Epoch 241/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9821 - loss: 0.1011 - val_accuracy: 0.6111 - val_loss: 2.1213\n",
      "Epoch 242/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9705 - loss: 0.1062 - val_accuracy: 0.6250 - val_loss: 2.1650\n",
      "Epoch 243/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9769 - loss: 0.0844 - val_accuracy: 0.6528 - val_loss: 2.0185\n",
      "Epoch 244/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9602 - loss: 0.1067 - val_accuracy: 0.6250 - val_loss: 2.0635\n",
      "Epoch 245/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9762 - loss: 0.0844 - val_accuracy: 0.5833 - val_loss: 2.0614\n",
      "Epoch 246/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9912 - loss: 0.0795 - val_accuracy: 0.6111 - val_loss: 1.9379\n",
      "Epoch 247/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9749 - loss: 0.1039 - val_accuracy: 0.6389 - val_loss: 1.9493\n",
      "Epoch 248/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9851 - loss: 0.0715 - val_accuracy: 0.6389 - val_loss: 2.0174\n",
      "Epoch 249/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9843 - loss: 0.0817 - val_accuracy: 0.6389 - val_loss: 2.0708\n",
      "Epoch 250/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9803 - loss: 0.0867 - val_accuracy: 0.6667 - val_loss: 2.1679\n",
      "Epoch 251/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9817 - loss: 0.0920 - val_accuracy: 0.6528 - val_loss: 2.1343\n",
      "Epoch 252/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9711 - loss: 0.0750 - val_accuracy: 0.6667 - val_loss: 2.1970\n",
      "Epoch 253/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9673 - loss: 0.0776 - val_accuracy: 0.6389 - val_loss: 2.0815\n",
      "Epoch 254/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9722 - loss: 0.0833 - val_accuracy: 0.6389 - val_loss: 2.1357\n",
      "Epoch 255/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9654 - loss: 0.0868 - val_accuracy: 0.6250 - val_loss: 2.1979\n",
      "Epoch 256/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9905 - loss: 0.0751 - val_accuracy: 0.6389 - val_loss: 1.9976\n",
      "Epoch 257/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9850 - loss: 0.0835 - val_accuracy: 0.6389 - val_loss: 2.0931\n",
      "Epoch 258/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9811 - loss: 0.0718 - val_accuracy: 0.6250 - val_loss: 1.9610\n",
      "Epoch 259/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9828 - loss: 0.0632 - val_accuracy: 0.5972 - val_loss: 2.0377\n",
      "Epoch 260/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9690 - loss: 0.1006 - val_accuracy: 0.6111 - val_loss: 2.3457\n",
      "Epoch 261/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9890 - loss: 0.0605 - val_accuracy: 0.6111 - val_loss: 2.2397\n",
      "Epoch 262/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9939 - loss: 0.0616 - val_accuracy: 0.5833 - val_loss: 2.0282\n",
      "Epoch 263/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9761 - loss: 0.0752 - val_accuracy: 0.5972 - val_loss: 2.1505\n",
      "Epoch 264/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9578 - loss: 0.1068 - val_accuracy: 0.6111 - val_loss: 2.1147\n",
      "Epoch 265/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9762 - loss: 0.0896 - val_accuracy: 0.6250 - val_loss: 2.1501\n",
      "Epoch 266/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9814 - loss: 0.0700 - val_accuracy: 0.6389 - val_loss: 2.1555\n",
      "Epoch 267/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9700 - loss: 0.0903 - val_accuracy: 0.6389 - val_loss: 2.0842\n",
      "Epoch 268/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9827 - loss: 0.1009 - val_accuracy: 0.6389 - val_loss: 2.0803\n",
      "Epoch 269/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9735 - loss: 0.0754 - val_accuracy: 0.6389 - val_loss: 2.0934\n",
      "Epoch 270/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9791 - loss: 0.0856 - val_accuracy: 0.6250 - val_loss: 2.1229\n",
      "Epoch 271/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9779 - loss: 0.0683 - val_accuracy: 0.6389 - val_loss: 2.3064\n",
      "Epoch 272/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9913 - loss: 0.0656 - val_accuracy: 0.6528 - val_loss: 2.0961\n",
      "Epoch 273/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9845 - loss: 0.0613 - val_accuracy: 0.6250 - val_loss: 2.0917\n",
      "Epoch 274/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9811 - loss: 0.0751 - val_accuracy: 0.6250 - val_loss: 2.1641\n",
      "Epoch 275/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9837 - loss: 0.0724 - val_accuracy: 0.5972 - val_loss: 2.2432\n",
      "Epoch 276/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9832 - loss: 0.0793 - val_accuracy: 0.6111 - val_loss: 2.0488\n",
      "Epoch 277/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9807 - loss: 0.0781 - val_accuracy: 0.6250 - val_loss: 2.1861\n",
      "Epoch 278/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9787 - loss: 0.0596 - val_accuracy: 0.6528 - val_loss: 2.2525\n",
      "Epoch 279/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9922 - loss: 0.0594 - val_accuracy: 0.6389 - val_loss: 2.3358\n",
      "Epoch 280/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9906 - loss: 0.0686 - val_accuracy: 0.6389 - val_loss: 2.0220\n",
      "Epoch 281/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9790 - loss: 0.0857 - val_accuracy: 0.6389 - val_loss: 2.0824\n",
      "Epoch 282/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9776 - loss: 0.0838 - val_accuracy: 0.6389 - val_loss: 2.2473\n",
      "Epoch 283/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9888 - loss: 0.0691 - val_accuracy: 0.6250 - val_loss: 2.3570\n",
      "Epoch 284/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9967 - loss: 0.0562 - val_accuracy: 0.6389 - val_loss: 2.3899\n",
      "Epoch 285/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9950 - loss: 0.0602 - val_accuracy: 0.5833 - val_loss: 2.5737\n",
      "Epoch 286/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9883 - loss: 0.0752 - val_accuracy: 0.6250 - val_loss: 2.2900\n",
      "Epoch 287/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9846 - loss: 0.0732 - val_accuracy: 0.6250 - val_loss: 2.2777\n",
      "Epoch 288/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9754 - loss: 0.1214 - val_accuracy: 0.6250 - val_loss: 2.2223\n",
      "Epoch 289/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9717 - loss: 0.0718 - val_accuracy: 0.6250 - val_loss: 2.3120\n",
      "Epoch 290/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9925 - loss: 0.0691 - val_accuracy: 0.6806 - val_loss: 2.3997\n",
      "Epoch 291/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9551 - loss: 0.1063 - val_accuracy: 0.6528 - val_loss: 2.2165\n",
      "Epoch 292/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9798 - loss: 0.0781 - val_accuracy: 0.6250 - val_loss: 2.1474\n",
      "Epoch 293/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9713 - loss: 0.0853 - val_accuracy: 0.6111 - val_loss: 2.1984\n",
      "Epoch 294/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9797 - loss: 0.0543 - val_accuracy: 0.6250 - val_loss: 2.0878\n",
      "Epoch 295/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9737 - loss: 0.0617 - val_accuracy: 0.6250 - val_loss: 2.1836\n",
      "Epoch 296/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9852 - loss: 0.0666 - val_accuracy: 0.5972 - val_loss: 2.1231\n",
      "Epoch 297/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9630 - loss: 0.0791 - val_accuracy: 0.5972 - val_loss: 2.0515\n",
      "Epoch 298/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9598 - loss: 0.1015 - val_accuracy: 0.5972 - val_loss: 2.0802\n",
      "Epoch 299/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9884 - loss: 0.0620 - val_accuracy: 0.5972 - val_loss: 2.1664\n",
      "Epoch 300/300\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9831 - loss: 0.0753 - val_accuracy: 0.6389 - val_loss: 2.1096\n",
      "Final Validation Loss: 2.109550952911377, Final Validation Accuracy: 0.6388888955116272\n",
      "Model, Tokenizer, and Classes Mapping Saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('updated_preprocessed_data.csv')\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X = data['input_text']\n",
    "y = data['target']\n",
    "\n",
    "# Create Tokenizer and fit on text data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert the target classes into a list and map them\n",
    "classes_set = list(set(y))\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes_set)}\n",
    "\n",
    "# Convert targets to integer labels\n",
    "y_encoded = y.map(class_to_index)\n",
    "\n",
    "# Prepare input sequences\n",
    "max_len = 20  # Max sequence length\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_padded, np.array(y_encoded), test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of X and y\n",
    "print(X_train.shape)  # Should be (num_samples, max_len)\n",
    "print(y_train.shape)  # Should be (num_samples,)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 10, input_length=max_len))\n",
    "np.random.seed(7)\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(classes_set), activation='softmax'))  # Number of unique classes\n",
    "\n",
    "# Compile the model with accuracy and precision\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Final evaluation\n",
    "# Final evaluation\n",
    "results = model.evaluate(X_val, y_val, verbose=0)\n",
    "loss = results[0]  # The first value is the loss\n",
    "accuracy = results[1]  # The second value is accuracy\n",
    "\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Final Validation Loss: {loss}, Final Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('Model_Results_test/my_model.keras')  # Saves the entire model in the .keras format\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "with open('Model_Results_test/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save the classes mapping using pickle\n",
    "with open('Model_Results_test/classes_set.pkl', 'wb') as f:\n",
    "    pickle.dump(classes_set, f)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model, Tokenizer, and Classes Mapping Saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 02:26:26.517988: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-16 02:26:26.518038: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: anudeep-OMEN-by-HP-Gaming-Laptop-16-wf1xxx\n",
      "2024-11-16 02:26:26.518050: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: anudeep-OMEN-by-HP-Gaming-Laptop-16-wf1xxx\n",
      "2024-11-16 02:26:26.518222: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 560.35.3\n",
      "2024-11-16 02:26:26.518247: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 560.35.3\n",
      "2024-11-16 02:26:26.518253: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:259] kernel version seems to match DSO: 560.35.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Tokenizer, and Classes Mapping Loaded!\n"
     ]
    }
   ],
   "source": [
    "## testing the things\n",
    "\n",
    "# Now you can later load the model and tokenizer like this:\n",
    "\n",
    "# 1. Load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "from CleanText import Token_to_Sent \n",
    "import pickle\n",
    "\n",
    "\n",
    "loaded_model = load_model('Model_Results_test/my_model.keras')\n",
    "\n",
    "max_len = 20\n",
    "\n",
    "# 2. Load the saved tokenizer\n",
    "with open('Model_Results_test/tokenizer.pkl', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "\n",
    "# 3. Load the classes mapping\n",
    "with open('Model_Results_test/classes_set.pkl', 'rb') as f:\n",
    "    loaded_classes_set = pickle.load(f)\n",
    "\n",
    "print(\"Model, Tokenizer, and Classes Mapping Loaded!\")\n",
    "\n",
    "# Function to optimize and predict new input\n",
    "def optimize_phrase(sentence):\n",
    "    sequences = loaded_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences\n",
    "\n",
    "# Example prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predicted_class(sentence):\n",
    "    refined_sentence = pipeline(input_phrase=sentence)\n",
    "    ts = Token_to_Sent() \n",
    "    optimized_input = optimize_phrase(ts.token_to_sent(tokenized_sentence=refined_sentence))\n",
    "    prediction = loaded_model.predict(optimized_input)\n",
    "    predicted_class = loaded_classes_set[np.argmax(prediction)]\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\") \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['my', 'ram', 'got', 'freeze']\n",
      "After stop words removal: ['ram', 'got', 'freeze']\n",
      "After stemming with porters algorithm: ['ram', 'got', 'freez']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted Class: RAM_frequency_issue\n",
      "RAM_frequency_issue\n"
     ]
    }
   ],
   "source": [
    "sentence = \"My ram got freeze\"  \n",
    "l = get_predicted_class(sentence) \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
